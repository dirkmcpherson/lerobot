{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DATA_DIR: None\n",
      "resume: false\n",
      "device: cuda\n",
      "use_amp: false\n",
      "seed: 100000\n",
      "dataset_repo_id: lerobot/pusht\n",
      "video_backend: pyav\n",
      "training:\n",
      "  offline_steps: 200000\n",
      "  online_steps: 0\n",
      "  online_steps_between_rollouts: 1\n",
      "  online_sampling_ratio: 0.5\n",
      "  online_env_seed: ???\n",
      "  eval_freq: 5000\n",
      "  log_freq: 250\n",
      "  save_checkpoint: true\n",
      "  save_freq: 5000\n",
      "  num_workers: 4\n",
      "  batch_size: 64\n",
      "  image_transforms:\n",
      "    enable: false\n",
      "    max_num_transforms: 3\n",
      "    random_order: false\n",
      "    brightness:\n",
      "      weight: 1\n",
      "      min_max:\n",
      "      - 0.8\n",
      "      - 1.2\n",
      "    contrast:\n",
      "      weight: 1\n",
      "      min_max:\n",
      "      - 0.8\n",
      "      - 1.2\n",
      "    saturation:\n",
      "      weight: 1\n",
      "      min_max:\n",
      "      - 0.5\n",
      "      - 1.5\n",
      "    hue:\n",
      "      weight: 1\n",
      "      min_max:\n",
      "      - -0.05\n",
      "      - 0.05\n",
      "    sharpness:\n",
      "      weight: 1\n",
      "      min_max:\n",
      "      - 0.8\n",
      "      - 1.2\n",
      "  grad_clip_norm: 10\n",
      "  lr: 0.0001\n",
      "  lr_scheduler: cosine\n",
      "  lr_warmup_steps: 500\n",
      "  adam_betas:\n",
      "  - 0.95\n",
      "  - 0.999\n",
      "  adam_eps: 1.0e-08\n",
      "  adam_weight_decay: 1.0e-06\n",
      "  delta_timestamps:\n",
      "    observation.image: '[i / ${fps} for i in range(1 - ${policy.n_obs_steps}, 1)]'\n",
      "    observation.state: '[i / ${fps} for i in range(1 - ${policy.n_obs_steps}, 1)]'\n",
      "    action: '[i / ${fps} for i in range(1 - ${policy.n_obs_steps}, 1 - ${policy.n_obs_steps}\n",
      "      + ${policy.horizon})]'\n",
      "  drop_n_last_frames: 7\n",
      "eval:\n",
      "  n_episodes: 50\n",
      "  batch_size: 50\n",
      "  use_async_envs: false\n",
      "wandb:\n",
      "  enable: false\n",
      "  disable_artifact: false\n",
      "  project: lerobot\n",
      "  notes: ''\n",
      "tensorboard:\n",
      "  enable: true\n",
      "fps: 10\n",
      "env:\n",
      "  name: pusht\n",
      "  task: PushT-v0\n",
      "  image_size: 96\n",
      "  state_dim: 2\n",
      "  action_dim: 2\n",
      "  fps: ${fps}\n",
      "  episode_length: 300\n",
      "  gym:\n",
      "    obs_type: pixels_agent_pos\n",
      "    render_mode: rgb_array\n",
      "    visualization_width: 384\n",
      "    visualization_height: 384\n",
      "override_dataset_stats:\n",
      "  observation.image:\n",
      "    mean:\n",
      "    - - - 0.5\n",
      "    - - - 0.5\n",
      "    - - - 0.5\n",
      "    std:\n",
      "    - - - 0.5\n",
      "    - - - 0.5\n",
      "    - - - 0.5\n",
      "  observation.state:\n",
      "    min:\n",
      "    - 13.456424\n",
      "    - 32.938293\n",
      "    max:\n",
      "    - 496.14618\n",
      "    - 510.9579\n",
      "  action:\n",
      "    min:\n",
      "    - 12.0\n",
      "    - 25.0\n",
      "    max:\n",
      "    - 511.0\n",
      "    - 511.0\n",
      "policy:\n",
      "  name: diffusion\n",
      "  n_obs_steps: 2\n",
      "  horizon: 16\n",
      "  n_action_steps: 8\n",
      "  input_shapes:\n",
      "    observation.image:\n",
      "    - 3\n",
      "    - 96\n",
      "    - 96\n",
      "    observation.state:\n",
      "    - ${env.state_dim}\n",
      "  output_shapes:\n",
      "    action:\n",
      "    - ${env.action_dim}\n",
      "  input_normalization_modes:\n",
      "    observation.image: mean_std\n",
      "    observation.state: min_max\n",
      "  output_normalization_modes:\n",
      "    action: min_max\n",
      "  vision_backbone: resnet18\n",
      "  crop_shape:\n",
      "  - 84\n",
      "  - 84\n",
      "  crop_is_random: true\n",
      "  pretrained_backbone_weights: null\n",
      "  use_group_norm: true\n",
      "  spatial_softmax_num_keypoints: 32\n",
      "  down_dims:\n",
      "  - 512\n",
      "  - 1024\n",
      "  - 2048\n",
      "  kernel_size: 5\n",
      "  n_groups: 8\n",
      "  diffusion_step_embed_dim: 128\n",
      "  use_film_scale_modulation: true\n",
      "  noise_scheduler_type: DDPM\n",
      "  num_train_timesteps: 100\n",
      "  beta_schedule: squaredcos_cap_v2\n",
      "  beta_start: 0.0001\n",
      "  beta_end: 0.02\n",
      "  prediction_type: epsilon\n",
      "  clip_sample: true\n",
      "  clip_sample_range: 1.0\n",
      "  num_inference_steps: null\n",
      "  do_mask_loss_for_padding: false\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "from pprint import pprint\n",
    "\n",
    "import lerobot\n",
    "from lerobot.common.datasets.lerobot_dataset import LeRobotDataset\n",
    "from lerobot.common.datasets.factory import make_dataset\n",
    "\n",
    "\n",
    "from hydra import compose, initialize\n",
    "from omegaconf import OmegaConf\n",
    "\n",
    "# context initialization\n",
    "with initialize(version_base=None, config_path=\"../configs\", job_name=\"test_app\"):\n",
    "    cfg = compose(config_name=\"default\")\n",
    "    print(OmegaConf.to_yaml(cfg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image (251, 64, 64, 3)\n",
      "reward (251,)\n",
      "is_first (251,)\n",
      "is_last (251,)\n",
      "is_terminal (251,)\n",
      "discount (251,)\n",
      "action (251, 5)\n",
      "logprob (251,)\n"
     ]
    }
   ],
   "source": [
    "# get the path to the dataset\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "env_name = 'pinpad' # 'robosuite'\n",
    "\n",
    "# base_path = Path(f\"~/workspace/lerobot/local/{env_name}/original\").expanduser()\n",
    "base_path = Path(f\"~/workspace/fastrl/logs/HD_pinpad_four_1/a\").expanduser()\n",
    "out_dir = Path(f\"~/workspace/lerobot/local/{env_name}\").expanduser()\n",
    "\n",
    "# list all the files in the dataset\n",
    "files = list(base_path.glob(\"*\"))\n",
    "# for f in files:\n",
    "#     print(f)\n",
    "\n",
    "# print the keys\n",
    "data = np.load(files[0])\n",
    "# convert to a dictionary NOTE: this is necessary to make the arrays writeable for some reason\n",
    "data = dict(data)\n",
    "for k,v in data.items():\n",
    "    print(k, v.shape)\n",
    "\n",
    "# print(\"Setting last is_terminal to true\")\n",
    "# data[\"is_terminal\"][-1] = True; data['is_last'][-1] = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing /home/james/workspace/fastrl/logs/HD_pinpad_four_1/a/20240526T083904-0e58eaa98531416ca3c6d59e51caac15-251.npz\n",
      "Processing /home/james/workspace/fastrl/logs/HD_pinpad_four_1/a/20240526T084033-2342665f3006441b96f5edbec383790f-251.npz\n",
      "Processing /home/james/workspace/fastrl/logs/HD_pinpad_four_1/a/20240526T084130-db32fa9047c947f99c1b94c53e5794e5-251.npz\n",
      "Processing /home/james/workspace/fastrl/logs/HD_pinpad_four_1/a/20240526T084150-ff264636787b4874b75e5483cd3b09c2-251.npz\n",
      "Processing /home/james/workspace/fastrl/logs/HD_pinpad_four_1/a/20240526T084333-a1cb4a900fb84d4b9ec9b7dd79094879-251.npz\n",
      "Processing /home/james/workspace/fastrl/logs/HD_pinpad_four_1/a/20240526T083956-a17de577b31d4331ae864c74985371fc-251.npz\n",
      "Processing /home/james/workspace/fastrl/logs/HD_pinpad_four_1/a/20240526T084230-8534bf771f704ce39c5d2ac2b131c8b8-251.npz\n",
      "Processing /home/james/workspace/fastrl/logs/HD_pinpad_four_1/a/20240526T084052-1d75be1f1c5845d4916c7a65ff100d6c-251.npz\n",
      "Processing /home/james/workspace/fastrl/logs/HD_pinpad_four_1/a/20240526T084111-06972c54e2444c6c98302348ba6863a9-251.npz\n",
      "Processing /home/james/workspace/fastrl/logs/HD_pinpad_four_1/a/20240526T083921-1946b112e9d64a279c2cf442015f804d-251.npz\n",
      "Processing /home/james/workspace/fastrl/logs/HD_pinpad_four_1/a/20240526T084210-03536c3091bf4dcd9cc6ef17917b06d3-251.npz\n",
      "Processing /home/james/workspace/fastrl/logs/HD_pinpad_four_1/a/20240526T084014-f9d26c35ce39456e94adbf0738edcafe-251.npz\n",
      "Processing /home/james/workspace/fastrl/logs/HD_pinpad_four_1/a/20240526T083939-85574b51477047e19b5c7730cf6be677-251.npz\n",
      "Processing /home/james/workspace/fastrl/logs/HD_pinpad_four_1/a/20240526T084251-69a1378594b14ef39c65a35120f6340b-251.npz\n",
      "Processing /home/james/workspace/fastrl/logs/HD_pinpad_four_1/a/20240526T084312-6172586b925c4c18a9a4236346281d45-251.npz\n",
      "image (3765, 64, 64, 3)\n",
      "reward (3765,)\n",
      "is_first (3765,)\n",
      "is_last (3765,)\n",
      "is_terminal (3765,)\n",
      "discount (3765,)\n",
      "action (3765, 5)\n",
      "logprob (3765,)\n"
     ]
    }
   ],
   "source": [
    "import tqdm\n",
    "import torch\n",
    "import einops\n",
    "import shutil\n",
    "from PIL import Image as PILImage\n",
    "import cv2\n",
    "\n",
    "from lerobot.common.datasets.push_dataset_to_hub.utils import concatenate_episodes, save_images_concurrently\n",
    "from lerobot.common.datasets.compute_stats import compute_stats\n",
    "from lerobot.scripts.push_dataset_to_hub import save_meta_data\n",
    "from lerobot.common.datasets.video_utils import VideoFrame, encode_video_frames\n",
    "from lerobot.common.datasets.utils import hf_transform_to_torch\n",
    "from datasets import Dataset, Features, Image, Sequence, Value\n",
    "\n",
    "def to_hf_dataset(data_dict, video):\n",
    "    features = {}\n",
    "\n",
    "    if video:\n",
    "        features[\"observation.image\"] = VideoFrame()\n",
    "    else:\n",
    "        features[\"observation.image\"] = Image()\n",
    "\n",
    "    features[\"observation.state\"] = Sequence(\n",
    "        length=data_dict[\"observation.state\"].shape[1], feature=Value(dtype=\"float32\", id=None)\n",
    "    )\n",
    "    features[\"action\"] = Sequence(\n",
    "        length=data_dict[\"action\"].shape[1], feature=Value(dtype=\"float32\", id=None)\n",
    "    )\n",
    "    features[\"episode_index\"] = Value(dtype=\"int64\", id=None)\n",
    "    features[\"frame_index\"] = Value(dtype=\"int64\", id=None)\n",
    "    features[\"timestamp\"] = Value(dtype=\"float32\", id=None)\n",
    "    features[\"next.reward\"] = Value(dtype=\"float32\", id=None)\n",
    "    features[\"next.done\"] = Value(dtype=\"bool\", id=None)\n",
    "    features[\"index\"] = Value(dtype=\"int64\", id=None)\n",
    "    # TODO(rcadene): add success\n",
    "    # features[\"next.success\"] = Value(dtype='bool', id=None)\n",
    "\n",
    "    hf_dataset = Dataset.from_dict(data_dict, features=Features(features))\n",
    "    hf_dataset.set_transform(hf_transform_to_torch)\n",
    "    return hf_dataset\n",
    "\n",
    "video = False; fps = 20; video_path = None\n",
    "debug = False\n",
    "\n",
    "\n",
    "ep_dicts = []\n",
    "episode_data_index = {\"from\": [], \"to\": []}\n",
    "\n",
    "id_from = 0\n",
    "id_to = 0\n",
    "ep_idx = 0\n",
    "\n",
    "data_dicts = []\n",
    "for data_fn in files:\n",
    "    print(f\"Processing {data_fn}\")\n",
    "    data = np.load(data_fn)\n",
    "    data = dict(data); \n",
    "    data[\"is_terminal\"][-1] = True\n",
    "    data_dicts.append(data)\n",
    "big_data_dict = {}\n",
    "for k in data_dicts[0].keys():\n",
    "    big_data_dict[k] = np.concatenate([d[k] for d in data_dicts], axis=0)\n",
    "    print(k, big_data_dict[k].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found terminal step\n",
      "found terminal step\n",
      "found terminal step\n",
      "found terminal step\n",
      "found terminal step\n",
      "found terminal step\n",
      "found terminal step\n",
      "found terminal step\n",
      "found terminal step\n",
      "found terminal step\n",
      "found terminal step\n",
      "found terminal step\n",
      "found terminal step\n",
      "found terminal step\n",
      "found terminal step\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "17d5976419e34012842222d0a597f1ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/3765 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LeRobotDataset(\n",
      "  Repository ID: 'pinpad',\n",
      "  Version: '0.0',\n",
      "  Split: 'train',\n",
      "  Number of Samples: 3765,\n",
      "  Number of Episodes: 15,\n",
      "  Type: image (.png),\n",
      "  Recorded Frames per Second: 20,\n",
      "  Camera Keys: ['observation.image'],\n",
      "  Video Frame Keys: N/A,\n",
      "  Transformations: None,\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Compute mean, min, max: 100%|█████████▉| 235/236 [00:01<00:00, 123.32it/s]\n",
      "Compute std: 100%|█████████▉| 235/236 [00:01<00:00, 125.06it/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "data = big_data_dict\n",
    "total_frames = data[\"action\"].shape[0]\n",
    "# for i in tqdm.tqdm(range(total_frames)):\n",
    "for i in range(total_frames):\n",
    "    id_to += 1\n",
    "\n",
    "    if not data[\"is_terminal\"][i]:\n",
    "        continue\n",
    "\n",
    "    print(\"found terminal step\")\n",
    "\n",
    "    num_frames = id_to - id_from\n",
    "\n",
    "    image = torch.tensor(data[\"image\"][id_from:id_to])\n",
    "    # image = einops.rearrange(image, \"b h w c -> b h w c\")\n",
    "    # image = einops.rearrange(image, \"b c h w -> b h w c\")\n",
    "    state = torch.tensor(data[\"vector_state\"][id_from:id_to]) if (\"vector_state\" in data) else torch.zeros(num_frames, 1)\n",
    "    action = torch.tensor(data[\"action\"][id_from:id_to])\n",
    "    # TODO(rcadene): we have a missing last frame which is the observation when the env is done\n",
    "    # it is critical to have this frame for tdmpc to predict a \"done observation/state\"\n",
    "    # next_image = torch.tensor(data[\"next_observations\"][\"rgb\"][id_from:id_to])\n",
    "    # next_state = torch.tensor(data[\"next_observations\"][\"state\"][id_from:id_to])\n",
    "    next_reward = torch.tensor(data[\"reward\"][id_from:id_to])\n",
    "    next_done = torch.tensor(data[\"is_terminal\"][id_from:id_to])\n",
    "\n",
    "    ep_dict = {}\n",
    "\n",
    "    imgs_array = [x.numpy() for x in image]\n",
    "    img_key = \"observation.image\"\n",
    "    if video:\n",
    "        # save png images in temporary directory\n",
    "        tmp_imgs_dir = out_dir / \"tmp_images\"\n",
    "        tmp_imgs_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        for i in range(len(imgs_array)):\n",
    "            img = PILImage.fromarray(imgs_array[i])\n",
    "            img.save(str(tmp_imgs_dir / f\"frame_{i:06d}.png\"), quality=100)\n",
    "\n",
    "        # encode images to a mp4 video\n",
    "        fname = f\"{img_key}_episode_{ep_idx:06d}.mp4\"\n",
    "        video_path = out_dir / \"videos\" / fname\n",
    "        encode_video_frames(tmp_imgs_dir, video_path, fps)\n",
    "\n",
    "        # clean temporary images directory\n",
    "        shutil.rmtree(tmp_imgs_dir)\n",
    "\n",
    "        # store the reference to the video frame\n",
    "        ep_dict[img_key] = [{\"path\": f\"videos/{fname}\", \"timestamp\": i / fps} for i in range(num_frames)]\n",
    "    else:\n",
    "        # ep_dict[img_key] = [PILImage.fromarray(x) for x in imgs_array]\n",
    "        ep_dict[img_key] = imgs_array\n",
    "\n",
    "    ep_dict[\"observation.state\"] = state\n",
    "    ep_dict[\"action\"] = action\n",
    "    ep_dict[\"episode_index\"] = torch.tensor([ep_idx] * num_frames, dtype=torch.int64)\n",
    "    ep_dict[\"frame_index\"] = torch.arange(0, num_frames, 1)\n",
    "    ep_dict[\"timestamp\"] = torch.arange(0, num_frames, 1) / fps\n",
    "    # ep_dict[\"next.observation.image\"] = next_image\n",
    "    # ep_dict[\"next.observation.state\"] = next_state\n",
    "    ep_dict[\"next.reward\"] = next_reward\n",
    "    ep_dict[\"next.done\"] = next_done\n",
    "    ep_dicts.append(ep_dict)\n",
    "\n",
    "    episode_data_index[\"from\"].append(id_from)\n",
    "    episode_data_index[\"to\"].append(id_from + num_frames)\n",
    "\n",
    "    id_from = id_to\n",
    "    ep_idx += 1\n",
    "\n",
    "    # process first episode only\n",
    "    if debug:\n",
    "        break\n",
    "if len(ep_dicts) == 0:\n",
    "    print(\"No terminal step found in the dataset\")\n",
    "else:\n",
    "    data_dict = concatenate_episodes(ep_dicts)\n",
    "    data_dict, episode_data_index\n",
    "\n",
    "    hf_dataset = to_hf_dataset(data_dict, video)\n",
    "\n",
    "    info = {\"fps\": fps, \"video\": video}\n",
    "\n",
    "    if video_path: \n",
    "        print(f\"video path: {video_path}\")\n",
    "    lerobot_dataset = LeRobotDataset.from_preloaded(\n",
    "    repo_id=\"pinpad\",\n",
    "    version=0.0,\n",
    "    hf_dataset=hf_dataset,\n",
    "    episode_data_index=episode_data_index,\n",
    "    info=info,\n",
    "    videos_dir=video_path,\n",
    "    )\n",
    "\n",
    "\n",
    "    hf_dataset = hf_dataset.with_format(None)  # to remove transforms that cant be saved\n",
    "    hf_dataset.save_to_disk(str(out_dir / \"train\"))\n",
    "print(lerobot_dataset)\n",
    "stats = compute_stats(lerobot_dataset, batch_size=16, num_workers=1)\n",
    "save_meta_data(info, stats, episode_data_index, out_dir / \"meta_data\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Caught TypeError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/home/james/workspace/fastrl/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py\", line 154, in collate\n    clone.update({key: collate([d[key] for d in batch], collate_fn_map=collate_fn_map) for key in elem})\n  File \"/home/james/workspace/fastrl/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py\", line 154, in <dictcomp>\n    clone.update({key: collate([d[key] for d in batch], collate_fn_map=collate_fn_map) for key in elem})\n  File \"/home/james/workspace/fastrl/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py\", line 191, in collate\n    raise TypeError(default_collate_err_msg_format.format(elem_type))\nTypeError: default_collate: batch must contain tensors, numpy arrays, numbers, dicts or lists; found <class 'PIL.PngImagePlugin.PngImageFile'>\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/home/james/workspace/fastrl/venv/lib/python3.10/site-packages/torch/utils/data/_utils/worker.py\", line 308, in _worker_loop\n    data = fetcher.fetch(index)  # type: ignore[possibly-undefined]\n  File \"/home/james/workspace/fastrl/venv/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py\", line 54, in fetch\n    return self.collate_fn(data)\n  File \"/home/james/workspace/fastrl/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py\", line 316, in default_collate\n    return collate(batch, collate_fn_map=default_collate_fn_map)\n  File \"/home/james/workspace/fastrl/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py\", line 161, in collate\n    return {key: collate([d[key] for d in batch], collate_fn_map=collate_fn_map) for key in elem}\n  File \"/home/james/workspace/fastrl/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py\", line 161, in <dictcomp>\n    return {key: collate([d[key] for d in batch], collate_fn_map=collate_fn_map) for key in elem}\n  File \"/home/james/workspace/fastrl/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py\", line 191, in collate\n    raise TypeError(default_collate_err_msg_format.format(elem_type))\nTypeError: default_collate: batch must contain tensors, numpy arrays, numbers, dicts or lists; found <class 'PIL.PngImagePlugin.PngImageFile'>\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 11\u001b[0m\n\u001b[1;32m      2\u001b[0m loaded_dataset \u001b[38;5;241m=\u001b[39m load_from_disk(\u001b[38;5;28mstr\u001b[39m(out_dir \u001b[38;5;241m/\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m      4\u001b[0m dataloader \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mDataLoader(\n\u001b[1;32m      5\u001b[0m     loaded_dataset,\n\u001b[1;32m      6\u001b[0m     num_workers\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m      7\u001b[0m     batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m,\n\u001b[1;32m      8\u001b[0m     shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m      9\u001b[0m )\n\u001b[0;32m---> 11\u001b[0m batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43miter\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdataloader\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/workspace/fastrl/venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py:631\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    628\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    630\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 631\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    633\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    635\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/workspace/fastrl/venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py:1346\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1344\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1345\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_task_info[idx]\n\u001b[0;32m-> 1346\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_process_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/workspace/fastrl/venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py:1372\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._process_data\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m   1370\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_try_put_index()\n\u001b[1;32m   1371\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, ExceptionWrapper):\n\u001b[0;32m-> 1372\u001b[0m     \u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreraise\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1373\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "File \u001b[0;32m~/workspace/fastrl/venv/lib/python3.10/site-packages/torch/_utils.py:705\u001b[0m, in \u001b[0;36mExceptionWrapper.reraise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    701\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m    702\u001b[0m     \u001b[38;5;66;03m# If the exception takes multiple arguments, don't try to\u001b[39;00m\n\u001b[1;32m    703\u001b[0m     \u001b[38;5;66;03m# instantiate since we don't know how to\u001b[39;00m\n\u001b[1;32m    704\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(msg) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 705\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m exception\n",
      "\u001b[0;31mTypeError\u001b[0m: Caught TypeError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/home/james/workspace/fastrl/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py\", line 154, in collate\n    clone.update({key: collate([d[key] for d in batch], collate_fn_map=collate_fn_map) for key in elem})\n  File \"/home/james/workspace/fastrl/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py\", line 154, in <dictcomp>\n    clone.update({key: collate([d[key] for d in batch], collate_fn_map=collate_fn_map) for key in elem})\n  File \"/home/james/workspace/fastrl/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py\", line 191, in collate\n    raise TypeError(default_collate_err_msg_format.format(elem_type))\nTypeError: default_collate: batch must contain tensors, numpy arrays, numbers, dicts or lists; found <class 'PIL.PngImagePlugin.PngImageFile'>\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/home/james/workspace/fastrl/venv/lib/python3.10/site-packages/torch/utils/data/_utils/worker.py\", line 308, in _worker_loop\n    data = fetcher.fetch(index)  # type: ignore[possibly-undefined]\n  File \"/home/james/workspace/fastrl/venv/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py\", line 54, in fetch\n    return self.collate_fn(data)\n  File \"/home/james/workspace/fastrl/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py\", line 316, in default_collate\n    return collate(batch, collate_fn_map=default_collate_fn_map)\n  File \"/home/james/workspace/fastrl/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py\", line 161, in collate\n    return {key: collate([d[key] for d in batch], collate_fn_map=collate_fn_map) for key in elem}\n  File \"/home/james/workspace/fastrl/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py\", line 161, in <dictcomp>\n    return {key: collate([d[key] for d in batch], collate_fn_map=collate_fn_map) for key in elem}\n  File \"/home/james/workspace/fastrl/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py\", line 191, in collate\n    raise TypeError(default_collate_err_msg_format.format(elem_type))\nTypeError: default_collate: batch must contain tensors, numpy arrays, numbers, dicts or lists; found <class 'PIL.PngImagePlugin.PngImageFile'>\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from datasets import load_dataset, load_from_disk\n",
    "loaded_dataset = load_from_disk(str(out_dir / \"train\"))\n",
    "\n",
    "dataloader = torch.utils.data.DataLoader(\n",
    "    loaded_dataset,\n",
    "    num_workers=1,\n",
    "    batch_size=2,\n",
    "    shuffle=False,\n",
    ")\n",
    "\n",
    "batch = next(iter(dataloader))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
