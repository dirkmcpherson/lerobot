{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert Datasets from gensis to lerobot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from pprint import pprint\n",
    "\n",
    "import lerobot\n",
    "from lerobot.common.datasets.lerobot_dataset import LeRobotDataset\n",
    "from lerobot.common.datasets.factory import make_dataset\n",
    "\n",
    "\n",
    "from hydra import compose, initialize\n",
    "from omegaconf import OmegaConf\n",
    "\n",
    "# context initialization\n",
    "with initialize(version_base=None, config_path=\"../configs\", job_name=\"test_app\"):\n",
    "    cfg = compose(config_name=\"default\")\n",
    "    print(OmegaConf.to_yaml(cfg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the path to the dataset\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "env_name = 'genesis' # 'ros' # 'pusht' # 'pinpad' # 'robosuite'\n",
    "imi = 0\n",
    "notes = 'eefS' #'diff_eef'\n",
    "\n",
    "# base_path = Path(f\"~/workspace/lerobot/local/{env_name}/original\").expanduser()\n",
    "# base_path = Path(f\"~/workspace/fastrl/logs/HD_pinpad_four_1/a\").expanduser()\n",
    "AI = False\n",
    "tdmpc = False\n",
    "USE_BOTTOM_IMAGE = False\n",
    "\n",
    "def get_files(env_name, imi, AI=False, tdmpc=False, resize=False):\n",
    "    if tdmpc:\n",
    "        bp = f\"~/workspace/fastrl/logs/demonstrations/TDMPC_pusht_HD_{imi}_sparse/\"\n",
    "        od = f\"~/workspace/lerobot/local/{env_name}/tdmpc{imi}\"\n",
    "        assert not AI\n",
    "    else:    \n",
    "        if AI:\n",
    "            bp = f\"~/workspace/fastrl/logs/AD_pusht_{imi}/\"\n",
    "            od = f\"~/workspace/lerobot/local/{env_name}/A{imi}\"\n",
    "        else:\n",
    "            bp = f\"~/workspace/fastrl/logs/HD_{env_name}_{imi}/\"\n",
    "            od = f\"~/workspace/lerobot/local/{env_name}/{imi}\"\n",
    "\n",
    "    if resize:\n",
    "        od = od + \"_96x96\"\n",
    "\n",
    "    base_path = Path(bp).expanduser()\n",
    "    out_dir = Path(od).expanduser()\n",
    "\n",
    "    print(base_path, end=\" \")\n",
    "    # list all the files in the dataset\n",
    "    folders = list(base_path.glob(\"*\")); print(f\"Found {len(folders)} folders\")\n",
    "\n",
    "    files = []\n",
    "    for f in folders:\n",
    "        files.extend((base_path / f).glob(\"*\"))\n",
    "    return files, out_dir\n",
    "\n",
    "files, out_dir = get_files(env_name, imi, AI=AI, tdmpc=tdmpc)\n",
    "\n",
    "print(files)\n",
    "\n",
    "# print the keys\n",
    "data = np.load(files[0])\n",
    "# convert to a dictionary NOTE: this is necessary to make the arrays writeable for some reason\n",
    "data = dict(data)\n",
    "for k,v in data.items():\n",
    "    print(k, v.shape)\n",
    "\n",
    "# print(\"Setting last is_terminal to true\")\n",
    "# data[\"is_terminal\"][-1] = True; data['is_last'][-1] = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "successful_runs_lengths = []\n",
    "all_actions = []\n",
    "all_states = []\n",
    "for f in files:\n",
    "    # read, convert to a dictionary NOTE: this is necessary to make the arrays writeable for some reason\n",
    "    data = np.load(f); data = dict(data)\n",
    "\n",
    "    # print(len(data['image_top']), np.sum([entry for entry in data['reward'] if entry > 0]), data['reward'][-1])\n",
    "\n",
    "    all_zero_actions = 0\n",
    "    if data['reward'][-1] > 0:\n",
    "        successful_runs_lengths.append(len(data['reward']) - 1)\n",
    "        # for a in data['action']:\n",
    "        #     if np.all(a == 0):\n",
    "        #         all_zero_actions += 1\n",
    "    all_actions.extend(data['action'])\n",
    "    all_states.extend(data['state'])\n",
    "\n",
    "    image_key = 'image'\n",
    "    # print(data[image_key].min(), data[image_key].max(), f'all zero actions: {all_zero_actions} / {len(data[\"action\"])}')\n",
    "    # print(data[image_key].min(), data[image_key].max())\n",
    "import matplotlib.pyplot as plt\n",
    "# plt.hist(successful_runs_lengths, bins=100, range=(0, 700))\n",
    "\n",
    "# make a hist of action dim 2 (z)\n",
    "plt.figure()\n",
    "\n",
    "all_actions = np.array(all_actions)\n",
    "for i in range(all_actions.shape[1]):\n",
    "    a = all_actions[:,i]\n",
    "    plt.hist(a, bins=100) # , range=(-0.5, 0.5))\n",
    "    plt.title(f\"Action dim {i}. mean: {np.mean(a):.2f}, std: {np.std(a):.2f}\")\n",
    "    plt.figure()\n",
    "\n",
    "CUTOFF = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_states = np.array(all_states)\n",
    "for i in range(all_states.shape[1]):\n",
    "    s = all_states[:,i]\n",
    "    plt.hist(s, bins=100) # , range=(-0.5, 0.5))\n",
    "    plt.title(f\"State dim {i}. mean: {np.mean(s):.2f}, std: {np.std(s):.2f}\")\n",
    "    plt.figure()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(all_actions, axis=0), np.std(all_actions, axis=0), np.min(all_actions, axis=0), np.max(all_actions, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lerobot.common.datasets.lerobot_dataset import LeRobotDataset, LeRobotDatasetMetadata\n",
    "import shutil\n",
    "repo_id = f\"j/{imi}\"\n",
    "root = Path(f'~/workspace/lerobot/local/{env_name}_{imi}_{notes}{CUTOFF}{\"_BOT\" if USE_BOTTOM_IMAGE else \"\"}').expanduser()\n",
    "dst = Path(f'~/.cache/huggingface/hub/datasets--{env_name}_{imi}_{notes}{CUTOFF}{\"\" if not USE_BOTTOM_IMAGE else \"_BOT\"}').expanduser()\n",
    "\n",
    "if dst.exists():\n",
    "    shutil.rmtree(dst)\n",
    "if root.exists():\n",
    "    shutil.rmtree(root)\n",
    "    \n",
    "# root.mkdir(exist_ok=True)\n",
    "# shutil.rmtree(root, ignore_errors=True)\n",
    "\n",
    "\n",
    "\n",
    "# loaded_dataset = LeRobotDataset(repo_id, dst, local_files_only=True)\n",
    "# loaded_dataset.num_episodes, loaded_dataset.num_frames\n",
    "# loaded_dataset.meta.stats\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "use_videos = True\n",
    "\n",
    "# h, w, ch = data['image_top'][0].shape\n",
    "img_shape = data['image'][0].shape\n",
    "ch = 3\n",
    "h, w = img_shape[:2]\n",
    "\n",
    "state_ndims = data['state'][0].shape[0]\n",
    "action_ndims = data['action'][0].shape[0]\n",
    "\n",
    "print(h, w, ch, state_ndims, action_ndims)\n",
    "\n",
    "features = {\n",
    "    \"observation.image.top\": {\n",
    "        \"dtype\": \"video\" if use_videos else \"image\",\n",
    "        \"shape\": [h, w, ch],\n",
    "        \"names\": ['height', 'width', 'channels'],\n",
    "        \"info\": None},\n",
    "    \"observation.state\": {\n",
    "        \"dtype\": \"float32\",\n",
    "        \"shape\": (state_ndims,),\n",
    "        \"names\": [f's{i}' for i in range(state_ndims)],\n",
    "    },\n",
    "    \"action\": {\n",
    "        \"dtype\": \"float32\",\n",
    "        \"shape\": (action_ndims,),\n",
    "        \"names\": [f'a{i}' for i in range(action_ndims)],\n",
    "    },\n",
    "    \"next.reward\": {\n",
    "        \"dtype\": \"float32\",\n",
    "        \"shape\": (1,),\n",
    "        \"names\": None,\n",
    "    },\n",
    "    \"next.success\": {\n",
    "        \"dtype\": \"bool\",\n",
    "        \"shape\": (1,),\n",
    "        \"names\": None,\n",
    "    },\n",
    "}\n",
    "\n",
    "if USE_BOTTOM_IMAGE:\n",
    "    features[\"observation.image.bottom\"] = {\n",
    "        \"dtype\": \"video\" if use_videos else \"image\",\n",
    "        \"shape\": [h, w, ch],\n",
    "        \"names\": ['height', 'width', 'channels'],\n",
    "        \"info\": None}\n",
    "\n",
    "# metadata = LeRobotDatasetMetadata(repo_id, root, local_files_only=True)\n",
    "dataset = LeRobotDataset.create(\n",
    "    repo_id,\n",
    "    fps=30, # from pusht.yaml\n",
    "    root=root,\n",
    "    use_videos=use_videos,\n",
    "    features=features\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lerobot.common.datasets.utils import DEFAULT_FEATURES\n",
    "\n",
    "mapping = {\n",
    "    'image': 'observation.image.top',\n",
    "    'state': 'observation.state',\n",
    "    'action': 'action',\n",
    "    'reward': 'next.reward',\n",
    "}\n",
    "\n",
    "if USE_BOTTOM_IMAGE:\n",
    "    mapping['image_bottom'] = 'observation.image.bottom'\n",
    "\n",
    "\n",
    "zero_action_frames_skipped = 0\n",
    "for f in files:\n",
    "    # read, convert to a dictionary NOTE: this is necessary to make the arrays writeable for some reason\n",
    "    data = np.load(f); data = dict(data)\n",
    "\n",
    "    if CUTOFF != '' and len(data['image']) > CUTOFF:\n",
    "        print(f\"Skipping {f} due to length {len(data['image'])}\")\n",
    "        continue\n",
    "\n",
    "    if np.sum(data['reward']) <= 0:\n",
    "        print(f\"Skipping {f} for no reward\")\n",
    "        continue\n",
    "\n",
    "    # skip if the last 10 frames of reward dont sum to > 10\n",
    "    if np.sum(data['reward'][-10:]) <= 10.0:\n",
    "        print(f\"Skipping {f} for no reward in last 10 frames\")\n",
    "        continue\n",
    "\n",
    "    nsteps = len(data['image'])\n",
    "    for t in range(nsteps):\n",
    "        frame = {}\n",
    "        for local_key, lerobot_key in mapping.items():\n",
    "            if \"image\" in lerobot_key:\n",
    "                # expand the grayscale image to 3 channels\n",
    "                img = data[local_key][t]\n",
    "                \n",
    "                # if the max of the images is above 1, normalize it\n",
    "                img = img.astype(np.float32)\n",
    "                if img.max() > 1:\n",
    "                    img = img / 255.0 # normalize to [0, 1]\n",
    "                \n",
    "                if len(img.shape) == 2:\n",
    "                    img = np.stack([img]*3, axis=-1)\n",
    "\n",
    "\n",
    "                frame[lerobot_key] = img\n",
    "            else:\n",
    "                frame[lerobot_key] = data[local_key][t]\n",
    "            # if local_key == 'action': \n",
    "                # frame[lerobot_key] = (frame[lerobot_key] + 1.) * 256 # NOTE: specifically for gym-pusht\n",
    "\n",
    "\n",
    "        frame['next.success'] = data['reward'][t] > 0\n",
    "\n",
    "        # Skip frames with all zero actions\n",
    "        if np.all(frame['action'] == 0):\n",
    "            zero_action_frames_skipped += 1\n",
    "            continue\n",
    "\n",
    "        dataset.add_frame(frame)\n",
    "    dataset.save_episode(\"Pick up a cup.\", encode_videos=False)\n",
    "\n",
    "print(f\"Zero action frames skipped: {zero_action_frames_skipped}\")\n",
    "dataset.consolidate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dataset.meta.stats['action']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "src = root\n",
    "dst = f'~/.cache/huggingface/hub/datasets--{env_name}_{imi}_{notes}{CUTOFF}{\"\" if not USE_BOTTOM_IMAGE else \"_BOT\"}'\n",
    "dst = Path(dst).expanduser()\n",
    "\n",
    "if Path(dst).exists():\n",
    "    shutil.rmtree(dst)\n",
    "shutil.copytree(src, dst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm\n",
    "import torch\n",
    "import einops\n",
    "import shutil\n",
    "from PIL import Image as PILImage\n",
    "import cv2\n",
    "\n",
    "from lerobot.common.datasets.push_dataset_to_hub.utils import concatenate_episodes, save_images_concurrently\n",
    "from lerobot.common.datasets.compute_stats import compute_stats\n",
    "from lerobot.scripts.push_dataset_to_hub import save_meta_data\n",
    "from lerobot.common.datasets.video_utils import VideoFrame, encode_video_frames\n",
    "from lerobot.common.datasets.utils import orhf_transform_to_torch\n",
    "from datasets import Dataset, Features, Image, Sequence, Value\n",
    "\n",
    "def to_hf_dataset(data_dict, video):\n",
    "    features = {}\n",
    "\n",
    "    if video:\n",
    "        features[\"observation.image\"] = VideoFrame()\n",
    "    else:\n",
    "        features[\"observation.image\"] = Image()\n",
    "\n",
    "    features[\"observation.state\"] = Sequence(\n",
    "        length=data_dict[\"observation.state\"].shape[1], feature=Value(dtype=\"float32\", id=None)\n",
    "    )\n",
    "    features[\"action\"] = Sequence(\n",
    "        length=data_dict[\"action\"].shape[1], feature=Value(dtype=\"float32\", id=None)\n",
    "    )\n",
    "    features[\"episode_index\"] = Value(dtype=\"int64\", id=None)\n",
    "    features[\"frame_index\"] = Value(dtype=\"int64\", id=None)\n",
    "    features[\"timestamp\"] = Value(dtype=\"float32\", id=None)\n",
    "    features[\"next.reward\"] = Value(dtype=\"float32\", id=None)\n",
    "    features[\"next.done\"] = Value(dtype=\"bool\", id=None)\n",
    "    features[\"index\"] = Value(dtype=\"int64\", id=None)\n",
    "    # TODO(rcadene): add success\n",
    "    # features[\"next.success\"] = Value(dtype='bool', id=None)\n",
    "\n",
    "    hf_dataset = Dataset.from_dict(data_dict, features=Features(features))\n",
    "    hf_dataset.set_transform(hf_transform_to_torch)\n",
    "    return hf_dataset\n",
    "\n",
    "def files_to_data_dict(files):\n",
    "    data_dicts = []\n",
    "    for data_fn in files:\n",
    "        print(f\"Processing {data_fn}\", end='...')\n",
    "        data = np.load(data_fn)\n",
    "        data = dict(data); \n",
    "        data[\"is_terminal\"][-1] = True\n",
    "        data_dicts.append(data)\n",
    "    print()\n",
    "    big_data_dict = {}\n",
    "    for k in data_dicts[0].keys():\n",
    "        big_data_dict[k] = np.concatenate([d[k] for d in data_dicts], axis=0)\n",
    "        print(k, big_data_dict[k].shape)\n",
    "        # if 'reward' in big_data_dict:\n",
    "        #     for kk in ['reward', 'is_terminal', 'is_last']:\n",
    "        #         print(f\"\\t{kk} {sum(big_data_dict[kk])}\", end='  ')\n",
    "    return big_data_dict\n",
    "\n",
    "# big_data_dict = files_to_data_dict(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fastrl_to_hf(big_data_dict, out_dir):\n",
    "    video = False; fps = 10; video_path = None; debug = False\n",
    "    ep_dicts = []\n",
    "    episode_data_index = {\"from\": [], \"to\": []}\n",
    "\n",
    "    id_from = 0\n",
    "    id_to = 0\n",
    "    ep_idx = 0\n",
    "    data = big_data_dict\n",
    "    total_frames = data[\"action\"].shape[0]\n",
    "# for i in tqdm.tqdm(range(total_frames)):\n",
    "    for i in range(total_frames):\n",
    "        id_to += 1\n",
    "\n",
    "        if not data[\"is_terminal\"][i]:\n",
    "            continue\n",
    "\n",
    "    # print(\"found terminal step\")\n",
    "\n",
    "        num_frames = id_to - id_from\n",
    "\n",
    "        image = torch.tensor(data[\"image\"][id_from:id_to])\n",
    "    # image = einops.rearrange(image, \"b h w c -> b h w c\")\n",
    "    # image = einops.rearrange(image, \"b c h w -> b h w c\")\n",
    "        state = torch.tensor(data[\"state\"][id_from:id_to, :2]) if (\"state\" in data) else torch.zeros(num_frames, 1)\n",
    "    # state = torch.tensor(data[\"vector_state\"][id_from:id_to]) if (\"vector_state\" in data) else torch.zeros(num_frames, 1)\n",
    "        action = (torch.tensor(data[\"action\"][id_from:id_to]) + 1) * 256\n",
    "    # action = torch.tensor(data[\"action\"][id_from:id_to])\n",
    "    # TODO(rcadene): we have a missing last frame which is the observation when the env is done\n",
    "    # it is critical to have this frame for tdmpc to predict a \"done observation/state\"\n",
    "    # next_image = torch.tensor(data[\"next_observations\"][\"rgb\"][id_from:id_to])\n",
    "    # next_state = torch.tensor(data[\"next_observations\"][\"state\"][id_from:id_to])\n",
    "        next_reward = torch.tensor(data[\"reward\"][id_from:id_to])\n",
    "        next_done = torch.tensor(data[\"is_terminal\"][id_from:id_to])\n",
    "\n",
    "        ep_dict = {}\n",
    "\n",
    "        imgs_array = [x.numpy() for x in image]\n",
    "        img_key = \"observation.image\"\n",
    "        if video:\n",
    "        # save png images in temporary directory\n",
    "            tmp_imgs_dir = out_dir / \"tmp_images\"\n",
    "            tmp_imgs_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "            for i in range(len(imgs_array)):\n",
    "                img = PILImage.fromarray(imgs_array[i])\n",
    "                img.save(str(tmp_imgs_dir / f\"frame_{i:06d}.png\"), quality=100)\n",
    "\n",
    "        # encode images to a mp4 video\n",
    "            fname = f\"{img_key}_episode_{ep_idx:06d}.mp4\"\n",
    "            video_path = out_dir / \"videos\" / fname\n",
    "            encode_video_frames(tmp_imgs_dir, video_path, fps)\n",
    "\n",
    "        # clean temporary images directory\n",
    "            shutil.rmtree(tmp_imgs_dir)\n",
    "\n",
    "        # store the reference to the video frame\n",
    "            ep_dict[img_key] = [{\"path\": f\"videos/{fname}\", \"timestamp\": i / fps} for i in range(num_frames)]\n",
    "        else:\n",
    "        # ep_dict[img_key] = [PILImage.fromarray(x) for x in imgs_array]\n",
    "            ep_dict[img_key] = imgs_array\n",
    "\n",
    "        ep_dict[\"observation.state\"] = state\n",
    "        ep_dict[\"action\"] = action\n",
    "        ep_dict[\"episode_index\"] = torch.tensor([ep_idx] * num_frames, dtype=torch.int64)\n",
    "        ep_dict[\"frame_index\"] = torch.arange(0, num_frames, 1)\n",
    "        ep_dict[\"timestamp\"] = torch.arange(0, num_frames, 1) / fps\n",
    "    # ep_dict[\"next.observation.image\"] = next_image\n",
    "    # ep_dict[\"next.observation.state\"] = next_state\n",
    "        ep_dict[\"next.reward\"] = next_reward\n",
    "        ep_dict[\"next.done\"] = next_done\n",
    "        ep_dicts.append(ep_dict)\n",
    "\n",
    "        episode_data_index[\"from\"].append(id_from)\n",
    "        episode_data_index[\"to\"].append(id_from + num_frames)\n",
    "\n",
    "        id_from = id_to\n",
    "        ep_idx += 1\n",
    "\n",
    "    # process first episode only\n",
    "        if debug:\n",
    "            break\n",
    "    if len(ep_dicts) == 0:\n",
    "        print(\"No terminal step found in the dataset\")\n",
    "    else:\n",
    "        for k,v in ep_dicts[0].items():\n",
    "            print(k, ep_dicts[0][k].shape if hasattr(ep_dicts[0][k], 'shape') else len(ep_dicts[0][k]), ep_dicts[-1][k].shape if hasattr(ep_dicts[-1][k], 'shape') else len(ep_dicts[-1][k]))\n",
    "\n",
    "        # convert things to\n",
    "        data_dict = concatenate_episodes(ep_dicts)\n",
    "        data_dict, episode_data_index\n",
    "\n",
    "        for k,v in data_dict.items():\n",
    "            print(k, v.shape if hasattr(v, 'shape') else len(v), type(v))\n",
    "\n",
    "        hf_dataset = to_hf_dataset(data_dict, video)\n",
    "\n",
    "        info = {\"fps\": fps, \"video\": video}\n",
    "\n",
    "        if video_path: \n",
    "            print(f\"video path: {video_path}\")\n",
    "        lerobot_dataset = LeRobotDataset.from_preloaded(\n",
    "            repo_id=env_name,\n",
    "            hf_dataset=hf_dataset,\n",
    "            episode_data_index=episode_data_index,\n",
    "            info=info,\n",
    "            videos_dir=video_path,\n",
    "            )\n",
    "\n",
    "\n",
    "        hf_dataset = hf_dataset.with_format(None)  # to remove transforms that cant be saved\n",
    "        hf_dataset.save_to_disk(str(out_dir / \"train\"))\n",
    "    # print(lerobot_dataset)\n",
    "    stats = compute_stats(lerobot_dataset, batch_size=16, num_workers=1)\n",
    "    save_meta_data(info, stats, episode_data_index, out_dir / \"meta_data\")\n",
    "    return stats\n",
    "\n",
    "# stats = fastrl_to_hf(big_data_dict, out_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "def resize_images(bdd):\n",
    "    for k in ['pixels', 'image']:\n",
    "        if k in bdd:\n",
    "            print(f\"Original {k} shape:\", bdd[k].shape)\n",
    "\n",
    "            # Reshape if necessary (assuming the images are in NHWC format)\n",
    "            if bdd[k].shape[-1] != 3:\n",
    "                bdd[k] = np.transpose(bdd[k], (0, 2, 3, 1))\n",
    "        \n",
    "            # Get the original dimensions\n",
    "            n, h, w, c = bdd[k].shape\n",
    "        \n",
    "            # Resize to 96x96\n",
    "            resized = np.zeros((n, 96, 96, c), dtype=bdd[k].dtype)\n",
    "            for i in range(n):\n",
    "                resized[i] = cv2.resize(bdd[k][i], (96, 96), interpolation=cv2.INTER_CUBIC)\n",
    "        \n",
    "        # Update the dictionary with resized images\n",
    "            bdd[k] = resized\n",
    "            \n",
    "            print(f\"Resized {k} shape:\", bdd[k].shape)\n",
    "    else:\n",
    "        print(f\"Key '{k}' not found in big_data_dict\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imis = [4, 5, 6, 7, 9, 10] if AI else [3,4,5,6,7,8,9,10]\n",
    "RESIZE_TO_96x96 = False\n",
    "imis = [22] #[11, 12, 13, 14]\n",
    "for ai_tag in [True, False]:\n",
    "    for imi in imis:\n",
    "        files, out_dir = get_files(env_name, imi, AI=ai_tag, resize=RESIZE_TO_96x96)\n",
    "        if files:\n",
    "            big_data_dict = files_to_data_dict(files)\n",
    "            if RESIZE_TO_96x96: resize_images(big_data_dict)\n",
    "            print(f\"Attempting to write to {out_dir}\")\n",
    "            stats = fastrl_to_hf(big_data_dict, out_dir)\n",
    "            # for k,v in stats.items():\n",
    "            #     print(k, v)\n",
    "        else: print(f\"Could not find files for {imi} AI {ai_tag}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "video = False; fps = 20; video_path = None; debug = False\n",
    "ep_dicts = []\n",
    "episode_data_index = {\"from\": [], \"to\": []}\n",
    "\n",
    "id_from = 0\n",
    "id_to = 0\n",
    "ep_idx = 0\n",
    "data = big_data_dict\n",
    "total_frames = data[\"action\"].shape[0]\n",
    "# for i in tqdm.tqdm(range(total_frames)):\n",
    "for i in range(total_frames):\n",
    "    id_to += 1\n",
    "\n",
    "    if not data[\"is_terminal\"][i]:\n",
    "        continue\n",
    "\n",
    "# print(\"found terminal step\")\n",
    "\n",
    "    num_frames = id_to - id_from\n",
    "\n",
    "    image = torch.tensor(data[\"image\"][id_from:id_to])\n",
    "# image = einops.rearrange(image, \"b h w c -> b h w c\")\n",
    "# image = einops.rearrange(image, \"b c h w -> b h w c\")\n",
    "    state = torch.tensor(data[\"state\"][id_from:id_to, :2]) if (\"state\" in data) else torch.zeros(num_frames, 1)\n",
    "# state = torch.tensor(data[\"vector_state\"][id_from:id_to]) if (\"vector_state\" in data) else torch.zeros(num_frames, 1)\n",
    "    action = (torch.tensor(data[\"action\"][id_from:id_to]) + 1) * 256\n",
    "# action = torch.tensor(data[\"action\"][id_from:id_to])\n",
    "# TODO(rcadene): we have a missing last frame which is the observation when the env is done\n",
    "# it is critical to have this frame for tdmpc to predict a \"done observation/state\"\n",
    "# next_image = torch.tensor(data[\"next_observations\"][\"rgb\"][id_from:id_to])\n",
    "# next_state = torch.tensor(data[\"next_observations\"][\"state\"][id_from:id_to])\n",
    "    next_reward = torch.tensor(data[\"reward\"][id_from:id_to])\n",
    "    next_done = torch.tensor(data[\"is_terminal\"][id_from:id_to])\n",
    "\n",
    "    ep_dict = {}\n",
    "\n",
    "    imgs_array = [x.numpy() for x in image]\n",
    "    img_key = \"observation.image\"\n",
    "    if video:\n",
    "    # save png images in temporary directory\n",
    "        tmp_imgs_dir = out_dir / \"tmp_images\"\n",
    "        tmp_imgs_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        for i in range(len(imgs_array)):\n",
    "            img = PILImage.fromarray(imgs_array[i])\n",
    "            img.save(str(tmp_imgs_dir / f\"frame_{i:06d}.png\"), quality=100)\n",
    "\n",
    "    # encode images to a mp4 video\n",
    "        fname = f\"{img_key}_episode_{ep_idx:06d}.mp4\"\n",
    "        video_path = out_dir / \"videos\" / fname\n",
    "        encode_video_frames(tmp_imgs_dir, video_path, fps)\n",
    "\n",
    "    # clean temporary images directory\n",
    "        shutil.rmtree(tmp_imgs_dir)\n",
    "\n",
    "    # store the reference to the video frame\n",
    "        ep_dict[img_key] = [{\"path\": f\"videos/{fname}\", \"timestamp\": i / fps} for i in range(num_frames)]\n",
    "    else:\n",
    "    # ep_dict[img_key] = [PILImage.fromarray(x) for x in imgs_array]\n",
    "        ep_dict[img_key] = imgs_array\n",
    "\n",
    "    ep_dict[\"observation.state\"] = state\n",
    "    ep_dict[\"action\"] = action\n",
    "    ep_dict[\"episode_index\"] = torch.tensor([ep_idx] * num_frames, dtype=torch.int64)\n",
    "    ep_dict[\"frame_index\"] = torch.arange(0, num_frames, 1)\n",
    "    ep_dict[\"timestamp\"] = torch.arange(0, num_frames, 1) / fps\n",
    "# ep_dict[\"next.observation.image\"] = next_image\n",
    "# ep_dict[\"next.observation.state\"] = next_state\n",
    "    ep_dict[\"next.reward\"] = next_reward\n",
    "    ep_dict[\"next.done\"] = next_done\n",
    "    ep_dicts.append(ep_dict)\n",
    "\n",
    "    episode_data_index[\"from\"].append(id_from)\n",
    "    episode_data_index[\"to\"].append(id_from + num_frames)\n",
    "\n",
    "    id_from = id_to\n",
    "    ep_idx += 1\n",
    "\n",
    "# process first episode only\n",
    "    if debug:\n",
    "        break\n",
    "if len(ep_dicts) == 0:\n",
    "    print(\"No terminal step found in the dataset\")\n",
    "else:\n",
    "    for k,v in ep_dicts[0].items():\n",
    "        print(k, ep_dicts[0][k].shape if hasattr(ep_dicts[0][k], 'shape') else len(ep_dicts[0][k]), ep_dicts[-1][k].shape if hasattr(ep_dicts[-1][k], 'shape') else len(ep_dicts[-1][k]))\n",
    "\n",
    "    # convert things to\n",
    "    data_dict = concatenate_episodes(ep_dicts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ep_dicts[0]['observation.image'][0].shape\n",
    "\n",
    "\n",
    "for f,t in zip(episode_data_index['from'], episode_data_index['to']):\n",
    "    print(f, t, data_dict['action'][f:t].shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.10.15)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
