{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cuda\n",
      "use_amp: false\n",
      "seed: 100000\n",
      "dataset_repo_id: lerobot/pusht\n",
      "training:\n",
      "  offline_steps: 200000\n",
      "  online_steps: 0\n",
      "  online_steps_between_rollouts: 1\n",
      "  online_sampling_ratio: 0.5\n",
      "  online_env_seed: ???\n",
      "  eval_freq: 5000\n",
      "  save_freq: 5000\n",
      "  log_freq: 250\n",
      "  save_model: true\n",
      "  batch_size: 64\n",
      "  grad_clip_norm: 10\n",
      "  lr: 0.0001\n",
      "  lr_scheduler: cosine\n",
      "  lr_warmup_steps: 500\n",
      "  adam_betas:\n",
      "  - 0.95\n",
      "  - 0.999\n",
      "  adam_eps: 1.0e-08\n",
      "  adam_weight_decay: 1.0e-06\n",
      "  delta_timestamps:\n",
      "    observation.image: '[i / ${fps} for i in range(1 - ${policy.n_obs_steps}, 1)]'\n",
      "    observation.state: '[i / ${fps} for i in range(1 - ${policy.n_obs_steps}, 1)]'\n",
      "    action: '[i / ${fps} for i in range(1 - ${policy.n_obs_steps}, 1 - ${policy.n_obs_steps}\n",
      "      + ${policy.horizon})]'\n",
      "eval:\n",
      "  n_episodes: 50\n",
      "  batch_size: 50\n",
      "  use_async_envs: false\n",
      "wandb:\n",
      "  enable: true\n",
      "  disable_artifact: false\n",
      "  project: lerobot\n",
      "  notes: ''\n",
      "tensorboard:\n",
      "  enable: true\n",
      "fps: 10\n",
      "env:\n",
      "  name: pusht\n",
      "  task: PushT-v0\n",
      "  from_pixels: true\n",
      "  pixels_only: false\n",
      "  image_size: 96\n",
      "  episode_length: 300\n",
      "  fps: ${fps}\n",
      "  state_dim: 2\n",
      "  action_dim: 2\n",
      "override_dataset_stats:\n",
      "  observation.image:\n",
      "    mean:\n",
      "    - - - 0.5\n",
      "    - - - 0.5\n",
      "    - - - 0.5\n",
      "    std:\n",
      "    - - - 0.5\n",
      "    - - - 0.5\n",
      "    - - - 0.5\n",
      "  observation.state:\n",
      "    min:\n",
      "    - 13.456424\n",
      "    - 32.938293\n",
      "    max:\n",
      "    - 496.14618\n",
      "    - 510.9579\n",
      "  action:\n",
      "    min:\n",
      "    - 12.0\n",
      "    - 25.0\n",
      "    max:\n",
      "    - 511.0\n",
      "    - 511.0\n",
      "policy:\n",
      "  name: diffusion\n",
      "  n_obs_steps: 2\n",
      "  horizon: 16\n",
      "  n_action_steps: 8\n",
      "  input_shapes:\n",
      "    observation.image:\n",
      "    - 3\n",
      "    - 96\n",
      "    - 96\n",
      "    observation.state:\n",
      "    - ${env.state_dim}\n",
      "  output_shapes:\n",
      "    action:\n",
      "    - ${env.action_dim}\n",
      "  input_normalization_modes:\n",
      "    observation.image: mean_std\n",
      "    observation.state: min_max\n",
      "  output_normalization_modes:\n",
      "    action: min_max\n",
      "  vision_backbone: resnet18\n",
      "  crop_shape:\n",
      "  - 84\n",
      "  - 84\n",
      "  crop_is_random: true\n",
      "  pretrained_backbone_weights: null\n",
      "  use_group_norm: true\n",
      "  spatial_softmax_num_keypoints: 32\n",
      "  down_dims:\n",
      "  - 512\n",
      "  - 1024\n",
      "  - 2048\n",
      "  kernel_size: 5\n",
      "  n_groups: 8\n",
      "  diffusion_step_embed_dim: 128\n",
      "  use_film_scale_modulation: true\n",
      "  noise_scheduler_type: DDPM\n",
      "  num_train_timesteps: 100\n",
      "  beta_schedule: squaredcos_cap_v2\n",
      "  beta_start: 0.0001\n",
      "  beta_end: 0.02\n",
      "  prediction_type: epsilon\n",
      "  clip_sample: true\n",
      "  clip_sample_range: 1.0\n",
      "  num_inference_steps: 100\n",
      "  do_mask_loss_for_padding: false\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "from pprint import pprint\n",
    "\n",
    "import lerobot\n",
    "from lerobot.common.datasets.lerobot_dataset import LeRobotDataset\n",
    "from lerobot.common.datasets.factory import make_dataset\n",
    "\n",
    "\n",
    "from hydra import compose, initialize\n",
    "from omegaconf import OmegaConf\n",
    "\n",
    "# context initialization\n",
    "with initialize(version_base=None, config_path=\"../configs\", job_name=\"test_app\"):\n",
    "    cfg = compose(config_name=\"default\")\n",
    "    print(OmegaConf.to_yaml(cfg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image (1001, 64, 64, 3)\n",
      "reward (1001,)\n",
      "is_first (1001,)\n",
      "is_last (1001,)\n",
      "is_terminal (1001,)\n",
      "discount (1001,)\n",
      "action (1001, 5)\n",
      "logprob (1001,)\n",
      "Setting last is_terminal to true\n"
     ]
    }
   ],
   "source": [
    "# get the path to the dataset\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "base_path = Path(\"~/workspace/lerobot/local/pinpad/original\").expanduser()\n",
    "out_dir = Path(\"~/workspace/lerobot/local/pinpad\").expanduser()\n",
    "\n",
    "# list all the files in the dataset\n",
    "files = list(base_path.glob(\"*\"))\n",
    "# for f in files:\n",
    "#     print(f)\n",
    "\n",
    "# print the keys\n",
    "data = np.load(files[0])\n",
    "# convert to a dictionary NOTE: this is necessary to make the arrays writeable for some reason\n",
    "data = dict(data)\n",
    "for k,v in data.items():\n",
    "    print(k, v.shape)\n",
    "\n",
    "print(\"Setting last is_terminal to true\")\n",
    "data[\"is_terminal\"][-1] = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1001/1001 [00:00<00:00, 77599.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found terminal step\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Keys mismatch: between {'observation.image': Image(mode=None, decode=True, id=None), 'observation.state': Sequence(feature=Value(dtype='float32', id=None), length=1, id=None), 'action': Sequence(feature=Value(dtype='float32', id=None), length=5, id=None), 'episode_index': Value(dtype='int64', id=None), 'frame_index': Value(dtype='int64', id=None), 'timestamp': Value(dtype='float32', id=None), 'next.reward': Value(dtype='float32', id=None), 'next.done': Value(dtype='bool', id=None), 'index': Value(dtype='int64', id=None)} (source) and {'observation.state': Sequence(feature=Value(dtype='float32', id=None), length=1, id=None), 'action': Sequence(feature=Value(dtype='float32', id=None), length=5, id=None), 'episode_index': Value(dtype='int64', id=None), 'frame_index': Value(dtype='int64', id=None), 'timestamp': Value(dtype='float32', id=None), 'next.reward': Value(dtype='float32', id=None), 'next.done': Value(dtype='bool', id=None), 'index': Value(dtype='int64', id=None)} (target).\n{'observation.image'} are missing from target and set() are missing from source\nThe 'source' features come from dataset_info.json, and the 'target' ones are those of the dataset arrow file.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "File \u001b[0;32m~/workspace/lerobot/venv/lib/python3.10/site-packages/datasets/arrow_dataset.py:708\u001b[0m, in \u001b[0;36mDataset.__init__\u001b[0;34m(self, arrow_table, info, split, indices_table, fingerprint)\u001b[0m\n\u001b[1;32m    707\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 708\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minfo\u001b[39m.\u001b[39mfeatures \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49minfo\u001b[39m.\u001b[39;49mfeatures\u001b[39m.\u001b[39;49mreorder_fields_as(inferred_features)\n\u001b[1;32m    709\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mValueError\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/workspace/lerobot/venv/lib/python3.10/site-packages/datasets/features/features.py:2114\u001b[0m, in \u001b[0;36mFeatures.reorder_fields_as\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m   2112\u001b[0m         \u001b[39mreturn\u001b[39;00m source\n\u001b[0;32m-> 2114\u001b[0m \u001b[39mreturn\u001b[39;00m Features(recursive_reorder(\u001b[39mself\u001b[39;49m, other))\n",
      "File \u001b[0;32m~/workspace/lerobot/venv/lib/python3.10/site-packages/datasets/features/features.py:2103\u001b[0m, in \u001b[0;36mFeatures.reorder_fields_as.<locals>.recursive_reorder\u001b[0;34m(source, target, stack)\u001b[0m\n\u001b[1;32m   2098\u001b[0m     message \u001b[39m=\u001b[39m (\n\u001b[1;32m   2099\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mKeys mismatch: between \u001b[39m\u001b[39m{\u001b[39;00msource\u001b[39m}\u001b[39;00m\u001b[39m (source) and \u001b[39m\u001b[39m{\u001b[39;00mtarget\u001b[39m}\u001b[39;00m\u001b[39m (target).\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m   2100\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00msource\u001b[39m.\u001b[39mkeys()\u001b[39m-\u001b[39mtarget\u001b[39m.\u001b[39mkeys()\u001b[39m}\u001b[39;00m\u001b[39m are missing from target \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   2101\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mand \u001b[39m\u001b[39m{\u001b[39;00mtarget\u001b[39m.\u001b[39mkeys()\u001b[39m-\u001b[39msource\u001b[39m.\u001b[39mkeys()\u001b[39m}\u001b[39;00m\u001b[39m are missing from source\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m stack_position\n\u001b[1;32m   2102\u001b[0m     )\n\u001b[0;32m-> 2103\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(message)\n\u001b[1;32m   2104\u001b[0m \u001b[39mreturn\u001b[39;00m {key: recursive_reorder(source[key], target[key], stack \u001b[39m+\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\u001b[39m{\u001b[39;00mkey\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m) \u001b[39mfor\u001b[39;00m key \u001b[39min\u001b[39;00m target}\n",
      "\u001b[0;31mValueError\u001b[0m: Keys mismatch: between {'observation.image': Image(mode=None, decode=True, id=None), 'observation.state': Sequence(feature=Value(dtype='float32', id=None), length=1, id=None), 'action': Sequence(feature=Value(dtype='float32', id=None), length=5, id=None), 'episode_index': Value(dtype='int64', id=None), 'frame_index': Value(dtype='int64', id=None), 'timestamp': Value(dtype='float32', id=None), 'next.reward': Value(dtype='float32', id=None), 'next.done': Value(dtype='bool', id=None), 'index': Value(dtype='int64', id=None)} (source) and {'observation.state': Sequence(feature=Value(dtype='float32', id=None), length=1, id=None), 'action': Sequence(feature=Value(dtype='float32', id=None), length=5, id=None), 'episode_index': Value(dtype='int64', id=None), 'frame_index': Value(dtype='int64', id=None), 'timestamp': Value(dtype='float32', id=None), 'next.reward': Value(dtype='float32', id=None), 'next.done': Value(dtype='bool', id=None), 'index': Value(dtype='int64', id=None)} (target).\n{'observation.image'} are missing from target and set() are missing from source",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/home/j/workspace/lerobot/lerobot/scripts/local_dataset.ipynb Cell 3\u001b[0m line \u001b[0;36m1\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/j/workspace/lerobot/lerobot/scripts/local_dataset.ipynb#W2sZmlsZQ%3D%3D?line=119'>120</a>\u001b[0m data_dict \u001b[39m=\u001b[39m concatenate_episodes(ep_dicts)\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/j/workspace/lerobot/lerobot/scripts/local_dataset.ipynb#W2sZmlsZQ%3D%3D?line=120'>121</a>\u001b[0m data_dict, episode_data_index\n\u001b[0;32m--> <a href='vscode-notebook-cell:/home/j/workspace/lerobot/lerobot/scripts/local_dataset.ipynb#W2sZmlsZQ%3D%3D?line=122'>123</a>\u001b[0m hf_dataset \u001b[39m=\u001b[39m to_hf_dataset(data_dict, video)\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/j/workspace/lerobot/lerobot/scripts/local_dataset.ipynb#W2sZmlsZQ%3D%3D?line=123'>124</a>\u001b[0m info \u001b[39m=\u001b[39m {\u001b[39m\"\u001b[39m\u001b[39mfps\u001b[39m\u001b[39m\"\u001b[39m: fps, \u001b[39m\"\u001b[39m\u001b[39mvideo\u001b[39m\u001b[39m\"\u001b[39m: video}\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/j/workspace/lerobot/lerobot/scripts/local_dataset.ipynb#W2sZmlsZQ%3D%3D?line=125'>126</a>\u001b[0m hf_dataset \u001b[39m=\u001b[39m hf_dataset\u001b[39m.\u001b[39mwith_format(\u001b[39mNone\u001b[39;00m)  \u001b[39m# to remove transforms that cant be saved\u001b[39;00m\n",
      "\u001b[1;32m/home/j/workspace/lerobot/lerobot/scripts/local_dataset.ipynb Cell 3\u001b[0m line \u001b[0;36m3\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/j/workspace/lerobot/lerobot/scripts/local_dataset.ipynb#W2sZmlsZQ%3D%3D?line=32'>33</a>\u001b[0m features[\u001b[39m\"\u001b[39m\u001b[39mindex\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m Value(dtype\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mint64\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mid\u001b[39m\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/j/workspace/lerobot/lerobot/scripts/local_dataset.ipynb#W2sZmlsZQ%3D%3D?line=33'>34</a>\u001b[0m \u001b[39m# TODO(rcadene): add success\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/j/workspace/lerobot/lerobot/scripts/local_dataset.ipynb#W2sZmlsZQ%3D%3D?line=34'>35</a>\u001b[0m \u001b[39m# features[\"next.success\"] = Value(dtype='bool', id=None)\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/j/workspace/lerobot/lerobot/scripts/local_dataset.ipynb#W2sZmlsZQ%3D%3D?line=36'>37</a>\u001b[0m hf_dataset \u001b[39m=\u001b[39m Dataset\u001b[39m.\u001b[39;49mfrom_dict(data_dict, features\u001b[39m=\u001b[39;49mFeatures(features))\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/j/workspace/lerobot/lerobot/scripts/local_dataset.ipynb#W2sZmlsZQ%3D%3D?line=37'>38</a>\u001b[0m hf_dataset\u001b[39m.\u001b[39mset_transform(hf_transform_to_torch)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/j/workspace/lerobot/lerobot/scripts/local_dataset.ipynb#W2sZmlsZQ%3D%3D?line=38'>39</a>\u001b[0m \u001b[39mreturn\u001b[39;00m hf_dataset\n",
      "File \u001b[0;32m~/workspace/lerobot/venv/lib/python3.10/site-packages/datasets/arrow_dataset.py:976\u001b[0m, in \u001b[0;36mDataset.from_dict\u001b[0;34m(cls, mapping, features, info, split)\u001b[0m\n\u001b[1;32m    967\u001b[0m \u001b[39mif\u001b[39;00m info\u001b[39m.\u001b[39mfeatures \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    968\u001b[0m     info\u001b[39m.\u001b[39mfeatures \u001b[39m=\u001b[39m Features(\n\u001b[1;32m    969\u001b[0m         {\n\u001b[1;32m    970\u001b[0m             col: generate_from_arrow_type(data\u001b[39m.\u001b[39mtype)\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    974\u001b[0m         }\n\u001b[1;32m    975\u001b[0m     )\n\u001b[0;32m--> 976\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mcls\u001b[39;49m(pa_table, info\u001b[39m=\u001b[39;49minfo, split\u001b[39m=\u001b[39;49msplit)\n",
      "File \u001b[0;32m~/workspace/lerobot/venv/lib/python3.10/site-packages/datasets/arrow_dataset.py:710\u001b[0m, in \u001b[0;36mDataset.__init__\u001b[0;34m(self, arrow_table, info, split, indices_table, fingerprint)\u001b[0m\n\u001b[1;32m    708\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minfo\u001b[39m.\u001b[39mfeatures \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minfo\u001b[39m.\u001b[39mfeatures\u001b[39m.\u001b[39mreorder_fields_as(inferred_features)\n\u001b[1;32m    709\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mValueError\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m--> 710\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    711\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00me\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39mThe \u001b[39m\u001b[39m'\u001b[39m\u001b[39msource\u001b[39m\u001b[39m'\u001b[39m\u001b[39m features come from dataset_info.json, and the \u001b[39m\u001b[39m'\u001b[39m\u001b[39mtarget\u001b[39m\u001b[39m'\u001b[39m\u001b[39m ones are those of the dataset arrow file.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    712\u001b[0m         )\n\u001b[1;32m    714\u001b[0m \u001b[39m# Infer fingerprint if None\u001b[39;00m\n\u001b[1;32m    716\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_fingerprint \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[0;31mValueError\u001b[0m: Keys mismatch: between {'observation.image': Image(mode=None, decode=True, id=None), 'observation.state': Sequence(feature=Value(dtype='float32', id=None), length=1, id=None), 'action': Sequence(feature=Value(dtype='float32', id=None), length=5, id=None), 'episode_index': Value(dtype='int64', id=None), 'frame_index': Value(dtype='int64', id=None), 'timestamp': Value(dtype='float32', id=None), 'next.reward': Value(dtype='float32', id=None), 'next.done': Value(dtype='bool', id=None), 'index': Value(dtype='int64', id=None)} (source) and {'observation.state': Sequence(feature=Value(dtype='float32', id=None), length=1, id=None), 'action': Sequence(feature=Value(dtype='float32', id=None), length=5, id=None), 'episode_index': Value(dtype='int64', id=None), 'frame_index': Value(dtype='int64', id=None), 'timestamp': Value(dtype='float32', id=None), 'next.reward': Value(dtype='float32', id=None), 'next.done': Value(dtype='bool', id=None), 'index': Value(dtype='int64', id=None)} (target).\n{'observation.image'} are missing from target and set() are missing from source\nThe 'source' features come from dataset_info.json, and the 'target' ones are those of the dataset arrow file."
     ]
    }
   ],
   "source": [
    "import tqdm\n",
    "import torch\n",
    "import einops\n",
    "import shutil\n",
    "from PIL import Image as PILImage\n",
    "\n",
    "from lerobot.common.datasets.push_dataset_to_hub.utils import concatenate_episodes, save_images_concurrently\n",
    "from lerobot.common.datasets.push_dataset_to_hub.compute_stats import compute_stats\n",
    "from lerobot.scripts.push_dataset_to_hub import save_meta_data\n",
    "from lerobot.common.datasets.video_utils import VideoFrame, encode_video_frames\n",
    "from lerobot.common.datasets.utils import hf_transform_to_torch\n",
    "from datasets import Dataset, Features, Image, Sequence, Value\n",
    "\n",
    "def to_hf_dataset(data_dict, video):\n",
    "    features = {}\n",
    "\n",
    "    if video:\n",
    "        features[\"observation.image\"] = VideoFrame()\n",
    "    else:\n",
    "        features[\"observation.image\"] = Image()\n",
    "\n",
    "    features[\"observation.state\"] = Sequence(\n",
    "        length=data_dict[\"observation.state\"].shape[1], feature=Value(dtype=\"float32\", id=None)\n",
    "    )\n",
    "    features[\"action\"] = Sequence(\n",
    "        length=data_dict[\"action\"].shape[1], feature=Value(dtype=\"float32\", id=None)\n",
    "    )\n",
    "    features[\"episode_index\"] = Value(dtype=\"int64\", id=None)\n",
    "    features[\"frame_index\"] = Value(dtype=\"int64\", id=None)\n",
    "    features[\"timestamp\"] = Value(dtype=\"float32\", id=None)\n",
    "    features[\"next.reward\"] = Value(dtype=\"float32\", id=None)\n",
    "    features[\"next.done\"] = Value(dtype=\"bool\", id=None)\n",
    "    features[\"index\"] = Value(dtype=\"int64\", id=None)\n",
    "    # TODO(rcadene): add success\n",
    "    # features[\"next.success\"] = Value(dtype='bool', id=None)\n",
    "\n",
    "    hf_dataset = Dataset.from_dict(data_dict, features=Features(features))\n",
    "    hf_dataset.set_transform(hf_transform_to_torch)\n",
    "    return hf_dataset\n",
    "\n",
    "video = False; fps = 15\n",
    "debug = False\n",
    "\n",
    "\n",
    "ep_dicts = []\n",
    "episode_data_index = {\"from\": [], \"to\": []}\n",
    "\n",
    "id_from = 0\n",
    "id_to = 0\n",
    "ep_idx = 0\n",
    "total_frames = data[\"action\"].shape[0]\n",
    "for i in tqdm.tqdm(range(total_frames)):\n",
    "    id_to += 1\n",
    "\n",
    "    if not data[\"is_terminal\"][i]:\n",
    "        continue\n",
    "    print(\"found terminal step\")\n",
    "\n",
    "    num_frames = id_to - id_from\n",
    "\n",
    "    image = torch.tensor(data[\"image\"][id_from:id_to])\n",
    "    # image = einops.rearrange(image, \"b h w c -> b h w c\")\n",
    "    # image = einops.rearrange(image, \"b c h w -> b h w c\")\n",
    "    state = torch.zeros(num_frames, 1)\n",
    "    action = torch.tensor(data[\"action\"][id_from:id_to])\n",
    "    # TODO(rcadene): we have a missing last frame which is the observation when the env is done\n",
    "    # it is critical to have this frame for tdmpc to predict a \"done observation/state\"\n",
    "    # next_image = torch.tensor(data[\"next_observations\"][\"rgb\"][id_from:id_to])\n",
    "    # next_state = torch.tensor(data[\"next_observations\"][\"state\"][id_from:id_to])\n",
    "    next_reward = torch.tensor(data[\"reward\"][id_from:id_to])\n",
    "    next_done = torch.tensor(data[\"is_terminal\"][id_from:id_to])\n",
    "\n",
    "    ep_dict = {}\n",
    "\n",
    "    imgs_array = [x.numpy() for x in image]\n",
    "    img_key = \"observation.image\"\n",
    "    if video:\n",
    "        # save png images in temporary directory\n",
    "        tmp_imgs_dir = out_dir / \"tmp_images\"\n",
    "        save_images_concurrently(imgs_array, tmp_imgs_dir)\n",
    "\n",
    "        # encode images to a mp4 video\n",
    "        fname = f\"{img_key}_episode_{ep_idx:06d}.mp4\"\n",
    "        video_path = out_dir / \"videos\" / fname\n",
    "        encode_video_frames(tmp_imgs_dir, video_path, fps)\n",
    "\n",
    "        # clean temporary images directory\n",
    "        shutil.rmtree(tmp_imgs_dir)\n",
    "\n",
    "        # store the reference to the video frame\n",
    "        ep_dict[img_key] = [{\"path\": f\"videos/{fname}\", \"timestamp\": i / fps} for i in range(num_frames)]\n",
    "    else:\n",
    "        pass\n",
    "        # ep_dict[img_key] = [PILImage.fromarray(x) for x in imgs_array]\n",
    "        # ep_dict[img_key] = [Image.fromarray(x) for x in imgs_array]\n",
    "\n",
    "    ep_dict[\"observation.state\"] = state\n",
    "    ep_dict[\"action\"] = action\n",
    "    ep_dict[\"episode_index\"] = torch.tensor([ep_idx] * num_frames, dtype=torch.int64)\n",
    "    ep_dict[\"frame_index\"] = torch.arange(0, num_frames, 1)\n",
    "    ep_dict[\"timestamp\"] = torch.arange(0, num_frames, 1) / fps\n",
    "    # ep_dict[\"next.observation.image\"] = next_image\n",
    "    # ep_dict[\"next.observation.state\"] = next_state\n",
    "    ep_dict[\"next.reward\"] = next_reward\n",
    "    ep_dict[\"next.done\"] = next_done\n",
    "    ep_dicts.append(ep_dict)\n",
    "\n",
    "    episode_data_index[\"from\"].append(id_from)\n",
    "    episode_data_index[\"to\"].append(id_from + num_frames)\n",
    "\n",
    "    id_from = id_to\n",
    "    ep_idx += 1\n",
    "\n",
    "    # process first episode only\n",
    "    if debug:\n",
    "        break\n",
    "if len(ep_dicts) == 0:\n",
    "    print(\"No terminal step found in the dataset\")\n",
    "else:\n",
    "    data_dict = concatenate_episodes(ep_dicts)\n",
    "    data_dict, episode_data_index\n",
    "\n",
    "    hf_dataset = to_hf_dataset(data_dict, video)\n",
    "    info = {\"fps\": fps, \"video\": video}\n",
    "\n",
    "    hf_dataset = hf_dataset.with_format(None)  # to remove transforms that cant be saved\n",
    "\n",
    "    lerobot_dataset = LeRobotDataset.from_preloaded(\n",
    "    repo_id=\"pinpad\",\n",
    "    version=0.0,\n",
    "    hf_dataset=hf_dataset,\n",
    "    episode_data_index=episode_data_index,\n",
    "    info=info,\n",
    "    videos_dir=video_path,\n",
    "    )\n",
    "    stats = compute_stats(lerobot_dataset, 64, 1)\n",
    "\n",
    "    hf_dataset.save_to_disk(str(out_dir / \"train\"))\n",
    "    save_meta_data(info, stats, episode_data_index, str(out_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# write the dictionary to disk as a json file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from datasets import load_dataset, load_from_disk\n",
    "load_from_disk()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
