{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/j/workspace/lerobot/venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data dir None\n",
      "resume: false\n",
      "device: cuda\n",
      "use_amp: false\n",
      "seed: 100000\n",
      "dataset_repo_id: lerobot/pusht\n",
      "video_backend: pyav\n",
      "training:\n",
      "  offline_steps: 200000\n",
      "  num_workers: 4\n",
      "  batch_size: 64\n",
      "  eval_freq: 25000\n",
      "  log_freq: 200\n",
      "  save_checkpoint: true\n",
      "  save_freq: 100000\n",
      "  online_steps: 0\n",
      "  online_rollout_n_episodes: 1\n",
      "  online_rollout_batch_size: 1\n",
      "  online_steps_between_rollouts: 1\n",
      "  online_sampling_ratio: 0.5\n",
      "  online_env_seed: null\n",
      "  online_buffer_capacity: null\n",
      "  online_buffer_seed_size: 0\n",
      "  do_online_rollout_async: false\n",
      "  image_transforms:\n",
      "    enable: false\n",
      "    max_num_transforms: 3\n",
      "    random_order: false\n",
      "    brightness:\n",
      "      weight: 1\n",
      "      min_max:\n",
      "      - 0.8\n",
      "      - 1.2\n",
      "    contrast:\n",
      "      weight: 1\n",
      "      min_max:\n",
      "      - 0.8\n",
      "      - 1.2\n",
      "    saturation:\n",
      "      weight: 1\n",
      "      min_max:\n",
      "      - 0.5\n",
      "      - 1.5\n",
      "    hue:\n",
      "      weight: 1\n",
      "      min_max:\n",
      "      - -0.05\n",
      "      - 0.05\n",
      "    sharpness:\n",
      "      weight: 1\n",
      "      min_max:\n",
      "      - 0.8\n",
      "      - 1.2\n",
      "  grad_clip_norm: 10\n",
      "  lr: 0.0001\n",
      "  lr_scheduler: cosine\n",
      "  lr_warmup_steps: 500\n",
      "  adam_betas:\n",
      "  - 0.95\n",
      "  - 0.999\n",
      "  adam_eps: 1.0e-08\n",
      "  adam_weight_decay: 1.0e-06\n",
      "  delta_timestamps:\n",
      "    observation.image: '[i / ${fps} for i in range(1 - ${policy.n_obs_steps}, 1)]'\n",
      "    observation.state: '[i / ${fps} for i in range(1 - ${policy.n_obs_steps}, 1)]'\n",
      "    action: '[i / ${fps} for i in range(1 - ${policy.n_obs_steps}, 1 - ${policy.n_obs_steps}\n",
      "      + ${policy.horizon})]'\n",
      "  drop_n_last_frames: 7\n",
      "eval:\n",
      "  n_episodes: 50\n",
      "  batch_size: 50\n",
      "  use_async_envs: false\n",
      "wandb:\n",
      "  enable: true\n",
      "  disable_artifact: false\n",
      "  entity: jambotime\n",
      "  project: diffusion_pusht\n",
      "  notes: ''\n",
      "fps: 10\n",
      "env:\n",
      "  name: pusht\n",
      "  task: PushT-v0\n",
      "  image_size: 96\n",
      "  state_dim: 2\n",
      "  action_dim: 2\n",
      "  fps: ${fps}\n",
      "  episode_length: 300\n",
      "  gym:\n",
      "    obs_type: pixels_agent_pos\n",
      "    render_mode: rgb_array\n",
      "    visualization_width: 384\n",
      "    visualization_height: 384\n",
      "    observation_width: 96\n",
      "    observation_height: 96\n",
      "    force_sparse: false\n",
      "    display_cross: false\n",
      "override_dataset_stats:\n",
      "  observation.image:\n",
      "    mean:\n",
      "    - - - 0.5\n",
      "    - - - 0.5\n",
      "    - - - 0.5\n",
      "    std:\n",
      "    - - - 0.5\n",
      "    - - - 0.5\n",
      "    - - - 0.5\n",
      "  observation.state:\n",
      "    min:\n",
      "    - 13.456424\n",
      "    - 32.938293\n",
      "    max:\n",
      "    - 496.14618\n",
      "    - 510.9579\n",
      "  action:\n",
      "    min:\n",
      "    - 12.0\n",
      "    - 25.0\n",
      "    max:\n",
      "    - 511.0\n",
      "    - 511.0\n",
      "policy:\n",
      "  name: diffusion\n",
      "  n_obs_steps: 2\n",
      "  horizon: 16\n",
      "  n_action_steps: 8\n",
      "  input_shapes:\n",
      "    observation.image:\n",
      "    - 3\n",
      "    - 96\n",
      "    - 96\n",
      "    observation.state:\n",
      "    - ${env.state_dim}\n",
      "  output_shapes:\n",
      "    action:\n",
      "    - ${env.action_dim}\n",
      "  input_normalization_modes:\n",
      "    observation.image: mean_std\n",
      "    observation.state: min_max\n",
      "  output_normalization_modes:\n",
      "    action: min_max\n",
      "  vision_backbone: resnet18\n",
      "  crop_shape:\n",
      "  - 84\n",
      "  - 84\n",
      "  crop_is_random: true\n",
      "  pretrained_backbone_weights: null\n",
      "  use_group_norm: true\n",
      "  spatial_softmax_num_keypoints: 32\n",
      "  down_dims:\n",
      "  - 512\n",
      "  - 1024\n",
      "  - 2048\n",
      "  kernel_size: 5\n",
      "  n_groups: 8\n",
      "  diffusion_step_embed_dim: 128\n",
      "  use_film_scale_modulation: true\n",
      "  noise_scheduler_type: DDPM\n",
      "  num_train_timesteps: 100\n",
      "  beta_schedule: squaredcos_cap_v2\n",
      "  beta_start: 0.0001\n",
      "  beta_end: 0.02\n",
      "  prediction_type: epsilon\n",
      "  clip_sample: true\n",
      "  clip_sample_range: 1.0\n",
      "  num_inference_steps: null\n",
      "  do_mask_loss_for_padding: false\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "from pprint import pprint\n",
    "\n",
    "import lerobot\n",
    "from lerobot.common.datasets.lerobot_dataset import LeRobotDataset\n",
    "from lerobot.common.datasets.factory import make_dataset\n",
    "\n",
    "\n",
    "from hydra import compose, initialize\n",
    "from omegaconf import OmegaConf\n",
    "\n",
    "# context initialization\n",
    "with initialize(version_base=None, config_path=\"../configs\", job_name=\"test_app\"):\n",
    "    cfg = compose(config_name=\"default\")\n",
    "    print(OmegaConf.to_yaml(cfg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PosixPath('/home/j/workspace/fastrl/logs/demonstrations/TDMPC_pusht_HD_50_sparse/transfered/76.npz'), PosixPath('/home/j/workspace/fastrl/logs/demonstrations/TDMPC_pusht_HD_50_sparse/transfered/15.npz'), PosixPath('/home/j/workspace/fastrl/logs/demonstrations/TDMPC_pusht_HD_50_sparse/transfered/57.npz'), PosixPath('/home/j/workspace/fastrl/logs/demonstrations/TDMPC_pusht_HD_50_sparse/transfered/23.npz'), PosixPath('/home/j/workspace/fastrl/logs/demonstrations/TDMPC_pusht_HD_50_sparse/transfered/65.npz'), PosixPath('/home/j/workspace/fastrl/logs/demonstrations/TDMPC_pusht_HD_50_sparse/transfered/47.npz'), PosixPath('/home/j/workspace/fastrl/logs/demonstrations/TDMPC_pusht_HD_50_sparse/transfered/84.npz'), PosixPath('/home/j/workspace/fastrl/logs/demonstrations/TDMPC_pusht_HD_50_sparse/transfered/10.npz'), PosixPath('/home/j/workspace/fastrl/logs/demonstrations/TDMPC_pusht_HD_50_sparse/transfered/13.npz'), PosixPath('/home/j/workspace/fastrl/logs/demonstrations/TDMPC_pusht_HD_50_sparse/transfered/30.npz'), PosixPath('/home/j/workspace/fastrl/logs/demonstrations/TDMPC_pusht_HD_50_sparse/transfered/88.npz'), PosixPath('/home/j/workspace/fastrl/logs/demonstrations/TDMPC_pusht_HD_50_sparse/transfered/82.npz'), PosixPath('/home/j/workspace/fastrl/logs/demonstrations/TDMPC_pusht_HD_50_sparse/transfered/72.npz'), PosixPath('/home/j/workspace/fastrl/logs/demonstrations/TDMPC_pusht_HD_50_sparse/transfered/81.npz'), PosixPath('/home/j/workspace/fastrl/logs/demonstrations/TDMPC_pusht_HD_50_sparse/transfered/8.npz'), PosixPath('/home/j/workspace/fastrl/logs/demonstrations/TDMPC_pusht_HD_50_sparse/transfered/43.npz'), PosixPath('/home/j/workspace/fastrl/logs/demonstrations/TDMPC_pusht_HD_50_sparse/transfered/78.npz'), PosixPath('/home/j/workspace/fastrl/logs/demonstrations/TDMPC_pusht_HD_50_sparse/transfered/44.npz'), PosixPath('/home/j/workspace/fastrl/logs/demonstrations/TDMPC_pusht_HD_50_sparse/transfered/66.npz'), PosixPath('/home/j/workspace/fastrl/logs/demonstrations/TDMPC_pusht_HD_50_sparse/transfered/48.npz'), PosixPath('/home/j/workspace/fastrl/logs/demonstrations/TDMPC_pusht_HD_50_sparse/transfered/27.npz'), PosixPath('/home/j/workspace/fastrl/logs/demonstrations/TDMPC_pusht_HD_50_sparse/transfered/32.npz'), PosixPath('/home/j/workspace/fastrl/logs/demonstrations/TDMPC_pusht_HD_50_sparse/transfered/62.npz'), PosixPath('/home/j/workspace/fastrl/logs/demonstrations/TDMPC_pusht_HD_50_sparse/transfered/68.npz'), PosixPath('/home/j/workspace/fastrl/logs/demonstrations/TDMPC_pusht_HD_50_sparse/transfered/40.npz'), PosixPath('/home/j/workspace/fastrl/logs/demonstrations/TDMPC_pusht_HD_50_sparse/transfered/46.npz'), PosixPath('/home/j/workspace/fastrl/logs/demonstrations/TDMPC_pusht_HD_50_sparse/transfered/16.npz'), PosixPath('/home/j/workspace/fastrl/logs/demonstrations/TDMPC_pusht_HD_50_sparse/transfered/86.npz'), PosixPath('/home/j/workspace/fastrl/logs/demonstrations/TDMPC_pusht_HD_50_sparse/transfered/1.npz'), PosixPath('/home/j/workspace/fastrl/logs/demonstrations/TDMPC_pusht_HD_50_sparse/transfered/12.npz'), PosixPath('/home/j/workspace/fastrl/logs/demonstrations/TDMPC_pusht_HD_50_sparse/transfered/64.npz'), PosixPath('/home/j/workspace/fastrl/logs/demonstrations/TDMPC_pusht_HD_50_sparse/transfered/36.npz'), PosixPath('/home/j/workspace/fastrl/logs/demonstrations/TDMPC_pusht_HD_50_sparse/transfered/4.npz'), PosixPath('/home/j/workspace/fastrl/logs/demonstrations/TDMPC_pusht_HD_50_sparse/transfered/38.npz'), PosixPath('/home/j/workspace/fastrl/logs/demonstrations/TDMPC_pusht_HD_50_sparse/transfered/69.npz'), PosixPath('/home/j/workspace/fastrl/logs/demonstrations/TDMPC_pusht_HD_50_sparse/transfered/14.npz'), PosixPath('/home/j/workspace/fastrl/logs/demonstrations/TDMPC_pusht_HD_50_sparse/transfered/26.npz'), PosixPath('/home/j/workspace/fastrl/logs/demonstrations/TDMPC_pusht_HD_50_sparse/transfered/25.npz'), PosixPath('/home/j/workspace/fastrl/logs/demonstrations/TDMPC_pusht_HD_50_sparse/transfered/35.npz'), PosixPath('/home/j/workspace/fastrl/logs/demonstrations/TDMPC_pusht_HD_50_sparse/transfered/83.npz'), PosixPath('/home/j/workspace/fastrl/logs/demonstrations/TDMPC_pusht_HD_50_sparse/transfered/79.npz'), PosixPath('/home/j/workspace/fastrl/logs/demonstrations/TDMPC_pusht_HD_50_sparse/transfered/50.npz'), PosixPath('/home/j/workspace/fastrl/logs/demonstrations/TDMPC_pusht_HD_50_sparse/transfered/45.npz'), PosixPath('/home/j/workspace/fastrl/logs/demonstrations/TDMPC_pusht_HD_50_sparse/transfered/74.npz'), PosixPath('/home/j/workspace/fastrl/logs/demonstrations/TDMPC_pusht_HD_50_sparse/transfered/42.npz'), PosixPath('/home/j/workspace/fastrl/logs/demonstrations/TDMPC_pusht_HD_50_sparse/transfered/9.npz'), PosixPath('/home/j/workspace/fastrl/logs/demonstrations/TDMPC_pusht_HD_50_sparse/transfered/75.npz'), PosixPath('/home/j/workspace/fastrl/logs/demonstrations/TDMPC_pusht_HD_50_sparse/transfered/5.npz'), PosixPath('/home/j/workspace/fastrl/logs/demonstrations/TDMPC_pusht_HD_50_sparse/transfered/53.npz'), PosixPath('/home/j/workspace/fastrl/logs/demonstrations/TDMPC_pusht_HD_50_sparse/transfered/2.npz'), PosixPath('/home/j/workspace/fastrl/logs/demonstrations/TDMPC_pusht_HD_50_sparse/transfered/58.npz'), PosixPath('/home/j/workspace/fastrl/logs/demonstrations/TDMPC_pusht_HD_50_sparse/transfered/3.npz'), PosixPath('/home/j/workspace/fastrl/logs/demonstrations/TDMPC_pusht_HD_50_sparse/transfered/49.npz'), PosixPath('/home/j/workspace/fastrl/logs/demonstrations/TDMPC_pusht_HD_50_sparse/transfered/73.npz'), PosixPath('/home/j/workspace/fastrl/logs/demonstrations/TDMPC_pusht_HD_50_sparse/transfered/22.npz'), PosixPath('/home/j/workspace/fastrl/logs/demonstrations/TDMPC_pusht_HD_50_sparse/transfered/19.npz'), PosixPath('/home/j/workspace/fastrl/logs/demonstrations/TDMPC_pusht_HD_50_sparse/transfered/70.npz'), PosixPath('/home/j/workspace/fastrl/logs/demonstrations/TDMPC_pusht_HD_50_sparse/transfered/21.npz'), PosixPath('/home/j/workspace/fastrl/logs/demonstrations/TDMPC_pusht_HD_50_sparse/transfered/60.npz'), PosixPath('/home/j/workspace/fastrl/logs/demonstrations/TDMPC_pusht_HD_50_sparse/transfered/59.npz'), PosixPath('/home/j/workspace/fastrl/logs/demonstrations/TDMPC_pusht_HD_50_sparse/transfered/6.npz'), PosixPath('/home/j/workspace/fastrl/logs/demonstrations/TDMPC_pusht_HD_50_sparse/transfered/31.npz'), PosixPath('/home/j/workspace/fastrl/logs/demonstrations/TDMPC_pusht_HD_50_sparse/transfered/54.npz'), PosixPath('/home/j/workspace/fastrl/logs/demonstrations/TDMPC_pusht_HD_50_sparse/transfered/41.npz'), PosixPath('/home/j/workspace/fastrl/logs/demonstrations/TDMPC_pusht_HD_50_sparse/transfered/85.npz'), PosixPath('/home/j/workspace/fastrl/logs/demonstrations/TDMPC_pusht_HD_50_sparse/transfered/34.npz'), PosixPath('/home/j/workspace/fastrl/logs/demonstrations/TDMPC_pusht_HD_50_sparse/transfered/0.npz'), PosixPath('/home/j/workspace/fastrl/logs/demonstrations/TDMPC_pusht_HD_50_sparse/transfered/63.npz'), PosixPath('/home/j/workspace/fastrl/logs/demonstrations/TDMPC_pusht_HD_50_sparse/transfered/39.npz'), PosixPath('/home/j/workspace/fastrl/logs/demonstrations/TDMPC_pusht_HD_50_sparse/transfered/17.npz'), PosixPath('/home/j/workspace/fastrl/logs/demonstrations/TDMPC_pusht_HD_50_sparse/transfered/61.npz'), PosixPath('/home/j/workspace/fastrl/logs/demonstrations/TDMPC_pusht_HD_50_sparse/transfered/37.npz'), PosixPath('/home/j/workspace/fastrl/logs/demonstrations/TDMPC_pusht_HD_50_sparse/transfered/56.npz'), PosixPath('/home/j/workspace/fastrl/logs/demonstrations/TDMPC_pusht_HD_50_sparse/transfered/67.npz'), PosixPath('/home/j/workspace/fastrl/logs/demonstrations/TDMPC_pusht_HD_50_sparse/transfered/24.npz'), PosixPath('/home/j/workspace/fastrl/logs/demonstrations/TDMPC_pusht_HD_50_sparse/transfered/51.npz'), PosixPath('/home/j/workspace/fastrl/logs/demonstrations/TDMPC_pusht_HD_50_sparse/transfered/33.npz'), PosixPath('/home/j/workspace/fastrl/logs/demonstrations/TDMPC_pusht_HD_50_sparse/transfered/20.npz'), PosixPath('/home/j/workspace/fastrl/logs/demonstrations/TDMPC_pusht_HD_50_sparse/transfered/55.npz'), PosixPath('/home/j/workspace/fastrl/logs/demonstrations/TDMPC_pusht_HD_50_sparse/transfered/28.npz'), PosixPath('/home/j/workspace/fastrl/logs/demonstrations/TDMPC_pusht_HD_50_sparse/transfered/29.npz'), PosixPath('/home/j/workspace/fastrl/logs/demonstrations/TDMPC_pusht_HD_50_sparse/transfered/77.npz'), PosixPath('/home/j/workspace/fastrl/logs/demonstrations/TDMPC_pusht_HD_50_sparse/transfered/7.npz'), PosixPath('/home/j/workspace/fastrl/logs/demonstrations/TDMPC_pusht_HD_50_sparse/transfered/18.npz'), PosixPath('/home/j/workspace/fastrl/logs/demonstrations/TDMPC_pusht_HD_50_sparse/transfered/52.npz'), PosixPath('/home/j/workspace/fastrl/logs/demonstrations/TDMPC_pusht_HD_50_sparse/transfered/11.npz'), PosixPath('/home/j/workspace/fastrl/logs/demonstrations/TDMPC_pusht_HD_50_sparse/transfered/80.npz'), PosixPath('/home/j/workspace/fastrl/logs/demonstrations/TDMPC_pusht_HD_50_sparse/transfered/87.npz'), PosixPath('/home/j/workspace/fastrl/logs/demonstrations/TDMPC_pusht_HD_50_sparse/transfered/89.npz'), PosixPath('/home/j/workspace/fastrl/logs/demonstrations/TDMPC_pusht_HD_50_sparse/transfered/71.npz')]\n",
      "pixels (7280, 3, 64, 64)\n",
      "image (7280, 3, 64, 64)\n",
      "state (0,)\n",
      "is_first (7280,)\n",
      "is_last (7280,)\n",
      "is_terminal (7280,)\n",
      "reward (7280,)\n",
      "discount (0,)\n",
      "action (7280, 2)\n"
     ]
    }
   ],
   "source": [
    "# get the path to the dataset\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "env_name = 'pusht' # 'pinpad' # 'robosuite'\n",
    "\n",
    "# base_path = Path(f\"~/workspace/lerobot/local/{env_name}/original\").expanduser()\n",
    "# base_path = Path(f\"~/workspace/fastrl/logs/HD_pinpad_four_1/a\").expanduser()\n",
    "imi = 50\n",
    "AI = False\n",
    "tdmpc = True\n",
    "\n",
    "def get_files(env_name, imi, AI=False, tdmpc=False, resize=False):\n",
    "    if tdmpc:\n",
    "        bp = f\"~/workspace/fastrl/logs/demonstrations/TDMPC_pusht_HD_{imi}_sparse/\"\n",
    "        od = f\"~/workspace/lerobot/local/{env_name}/tdmpc{imi}\"\n",
    "        assert not AI\n",
    "    else:    \n",
    "        if AI:\n",
    "            bp = f\"~/workspace/fastrl/logs/AD_pusht_{imi}/\"\n",
    "            od = f\"~/workspace/lerobot/local/{env_name}/A{imi}\"\n",
    "        else:\n",
    "            bp = f\"~/workspace/fastrl/logs/HD_pusht_{imi}/\"\n",
    "            od = f\"~/workspace/lerobot/local/{env_name}/{imi}\"\n",
    "\n",
    "    if resize:\n",
    "        od = od + \"_96x96\"\n",
    "\n",
    "    base_path = Path(bp).expanduser()\n",
    "    out_dir = Path(od).expanduser()\n",
    "\n",
    "    # print(base_path)\n",
    "# list all the files in the dataset\n",
    "    folders = list(base_path.glob(\"*\"))\n",
    "\n",
    "    files = []\n",
    "    for f in folders:\n",
    "        files.extend((base_path / f).glob(\"*\"))\n",
    "    return files, out_dir\n",
    "\n",
    "files, out_dir = get_files(env_name, imi, AI=AI, tdmpc=tdmpc)\n",
    "\n",
    "print(files)\n",
    "\n",
    "# print the keys\n",
    "data = np.load(files[0])\n",
    "# convert to a dictionary NOTE: this is necessary to make the arrays writeable for some reason\n",
    "data = dict(data)\n",
    "for k,v in data.items():\n",
    "    print(k, v.shape)\n",
    "\n",
    "# print(\"Setting last is_terminal to true\")\n",
    "# data[\"is_terminal\"][-1] = True; data['is_last'][-1] = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm\n",
    "import torch\n",
    "import einops\n",
    "import shutil\n",
    "from PIL import Image as PILImage\n",
    "import cv2\n",
    "\n",
    "from lerobot.common.datasets.push_dataset_to_hub.utils import concatenate_episodes, save_images_concurrently\n",
    "from lerobot.common.datasets.compute_stats import compute_stats\n",
    "from lerobot.scripts.push_dataset_to_hub import save_meta_data\n",
    "from lerobot.common.datasets.video_utils import VideoFrame, encode_video_frames\n",
    "from lerobot.common.datasets.utils import hf_transform_to_torch\n",
    "from datasets import Dataset, Features, Image, Sequence, Value\n",
    "\n",
    "def to_hf_dataset(data_dict, video):\n",
    "    features = {}\n",
    "\n",
    "    if video:\n",
    "        features[\"observation.image\"] = VideoFrame()\n",
    "    else:\n",
    "        features[\"observation.image\"] = Image()\n",
    "\n",
    "    features[\"observation.state\"] = Sequence(\n",
    "        length=data_dict[\"observation.state\"].shape[1], feature=Value(dtype=\"float32\", id=None)\n",
    "    )\n",
    "    features[\"action\"] = Sequence(\n",
    "        length=data_dict[\"action\"].shape[1], feature=Value(dtype=\"float32\", id=None)\n",
    "    )\n",
    "    features[\"episode_index\"] = Value(dtype=\"int64\", id=None)\n",
    "    features[\"frame_index\"] = Value(dtype=\"int64\", id=None)\n",
    "    features[\"timestamp\"] = Value(dtype=\"float32\", id=None)\n",
    "    features[\"next.reward\"] = Value(dtype=\"float32\", id=None)\n",
    "    features[\"next.done\"] = Value(dtype=\"bool\", id=None)\n",
    "    features[\"index\"] = Value(dtype=\"int64\", id=None)\n",
    "    # TODO(rcadene): add success\n",
    "    # features[\"next.success\"] = Value(dtype='bool', id=None)\n",
    "\n",
    "    hf_dataset = Dataset.from_dict(data_dict, features=Features(features))\n",
    "    hf_dataset.set_transform(hf_transform_to_torch)\n",
    "    return hf_dataset\n",
    "\n",
    "def files_to_data_dict(files):\n",
    "    data_dicts = []\n",
    "    for data_fn in files:\n",
    "        print(f\"Processing {data_fn}\")\n",
    "        data = np.load(data_fn)\n",
    "        data = dict(data); \n",
    "        data[\"is_terminal\"][-1] = True\n",
    "        data_dicts.append(data)\n",
    "    big_data_dict = {}\n",
    "    for k in data_dicts[0].keys():\n",
    "        big_data_dict[k] = np.concatenate([d[k] for d in data_dicts], axis=0)\n",
    "        print(k, big_data_dict[k].shape)\n",
    "        if 'reward' in big_data_dict:\n",
    "            for kk in ['reward', 'is_terminal', 'is_last']:\n",
    "                print(f\"\\t{kk} {sum(big_data_dict[kk])}\", end='  ')\n",
    "    return big_data_dict\n",
    "\n",
    "# big_data_dict = files_to_data_dict(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# img = big_data_dict['image'][100]\n",
    "# from matplotlib import pyplot as plt\n",
    "# # img = np.random.random((64, 64, 3))\n",
    "# for img in big_data_dict['image'][:100]:\n",
    "#     plt.imshow(img, interpolation='nearest')\n",
    "#     plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fastrl_to_hf(big_data_dict, out_dir):\n",
    "    video = False; fps = 10; video_path = None; debug = False\n",
    "    ep_dicts = []\n",
    "    episode_data_index = {\"from\": [], \"to\": []}\n",
    "\n",
    "    id_from = 0\n",
    "    id_to = 0\n",
    "    ep_idx = 0\n",
    "    data = big_data_dict\n",
    "    total_frames = data[\"action\"].shape[0]\n",
    "# for i in tqdm.tqdm(range(total_frames)):\n",
    "    for i in range(total_frames):\n",
    "        id_to += 1\n",
    "\n",
    "        if not data[\"is_terminal\"][i]:\n",
    "            continue\n",
    "\n",
    "    # print(\"found terminal step\")\n",
    "\n",
    "        num_frames = id_to - id_from\n",
    "\n",
    "        image = torch.tensor(data[\"image\"][id_from:id_to])\n",
    "    # image = einops.rearrange(image, \"b h w c -> b h w c\")\n",
    "    # image = einops.rearrange(image, \"b c h w -> b h w c\")\n",
    "        state = torch.tensor(data[\"state\"][id_from:id_to, :2]) if (\"state\" in data) else torch.zeros(num_frames, 1)\n",
    "    # state = torch.tensor(data[\"vector_state\"][id_from:id_to]) if (\"vector_state\" in data) else torch.zeros(num_frames, 1)\n",
    "        action = (torch.tensor(data[\"action\"][id_from:id_to]) + 1) * 256\n",
    "    # action = torch.tensor(data[\"action\"][id_from:id_to])\n",
    "    # TODO(rcadene): we have a missing last frame which is the observation when the env is done\n",
    "    # it is critical to have this frame for tdmpc to predict a \"done observation/state\"\n",
    "    # next_image = torch.tensor(data[\"next_observations\"][\"rgb\"][id_from:id_to])\n",
    "    # next_state = torch.tensor(data[\"next_observations\"][\"state\"][id_from:id_to])\n",
    "        next_reward = torch.tensor(data[\"reward\"][id_from:id_to])\n",
    "        next_done = torch.tensor(data[\"is_terminal\"][id_from:id_to])\n",
    "\n",
    "        ep_dict = {}\n",
    "\n",
    "        imgs_array = [x.numpy() for x in image]\n",
    "        img_key = \"observation.image\"\n",
    "        if video:\n",
    "        # save png images in temporary directory\n",
    "            tmp_imgs_dir = out_dir / \"tmp_images\"\n",
    "            tmp_imgs_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "            for i in range(len(imgs_array)):\n",
    "                img = PILImage.fromarray(imgs_array[i])\n",
    "                img.save(str(tmp_imgs_dir / f\"frame_{i:06d}.png\"), quality=100)\n",
    "\n",
    "        # encode images to a mp4 video\n",
    "            fname = f\"{img_key}_episode_{ep_idx:06d}.mp4\"\n",
    "            video_path = out_dir / \"videos\" / fname\n",
    "            encode_video_frames(tmp_imgs_dir, video_path, fps)\n",
    "\n",
    "        # clean temporary images directory\n",
    "            shutil.rmtree(tmp_imgs_dir)\n",
    "\n",
    "        # store the reference to the video frame\n",
    "            ep_dict[img_key] = [{\"path\": f\"videos/{fname}\", \"timestamp\": i / fps} for i in range(num_frames)]\n",
    "        else:\n",
    "        # ep_dict[img_key] = [PILImage.fromarray(x) for x in imgs_array]\n",
    "            ep_dict[img_key] = imgs_array\n",
    "\n",
    "        ep_dict[\"observation.state\"] = state\n",
    "        ep_dict[\"action\"] = action\n",
    "        ep_dict[\"episode_index\"] = torch.tensor([ep_idx] * num_frames, dtype=torch.int64)\n",
    "        ep_dict[\"frame_index\"] = torch.arange(0, num_frames, 1)\n",
    "        ep_dict[\"timestamp\"] = torch.arange(0, num_frames, 1) / fps\n",
    "    # ep_dict[\"next.observation.image\"] = next_image\n",
    "    # ep_dict[\"next.observation.state\"] = next_state\n",
    "        ep_dict[\"next.reward\"] = next_reward\n",
    "        ep_dict[\"next.done\"] = next_done\n",
    "        ep_dicts.append(ep_dict)\n",
    "\n",
    "        episode_data_index[\"from\"].append(id_from)\n",
    "        episode_data_index[\"to\"].append(id_from + num_frames)\n",
    "\n",
    "        id_from = id_to\n",
    "        ep_idx += 1\n",
    "\n",
    "    # process first episode only\n",
    "        if debug:\n",
    "            break\n",
    "    if len(ep_dicts) == 0:\n",
    "        print(\"No terminal step found in the dataset\")\n",
    "    else:\n",
    "        for k,v in ep_dicts[0].items():\n",
    "            print(k, ep_dicts[0][k].shape if hasattr(ep_dicts[0][k], 'shape') else len(ep_dicts[0][k]), ep_dicts[-1][k].shape if hasattr(ep_dicts[-1][k], 'shape') else len(ep_dicts[-1][k]))\n",
    "\n",
    "        # convert things to\n",
    "        data_dict = concatenate_episodes(ep_dicts)\n",
    "        data_dict, episode_data_index\n",
    "\n",
    "        for k,v in data_dict.items():\n",
    "            print(k, v.shape if hasattr(v, 'shape') else len(v))\n",
    "\n",
    "        hf_dataset = to_hf_dataset(data_dict, video)\n",
    "\n",
    "        info = {\"fps\": fps, \"video\": video}\n",
    "\n",
    "        if video_path: \n",
    "            print(f\"video path: {video_path}\")\n",
    "        lerobot_dataset = LeRobotDataset.from_preloaded(\n",
    "            repo_id=env_name,\n",
    "            hf_dataset=hf_dataset,\n",
    "            episode_data_index=episode_data_index,\n",
    "            info=info,\n",
    "            videos_dir=video_path,\n",
    "            )\n",
    "\n",
    "\n",
    "        hf_dataset = hf_dataset.with_format(None)  # to remove transforms that cant be saved\n",
    "        hf_dataset.save_to_disk(str(out_dir / \"train\"))\n",
    "    # print(lerobot_dataset)\n",
    "    stats = compute_stats(lerobot_dataset, batch_size=16, num_workers=1)\n",
    "    save_meta_data(info, stats, episode_data_index, out_dir / \"meta_data\")\n",
    "    return stats\n",
    "\n",
    "# stats = fastrl_to_hf(big_data_dict, out_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conversion from tdmpc to fastrl format does not work as of 10/6/24\n",
    "# # imis = [4, 5, 6, 7, 9, 10] if AI else [3,4,5,6,7,8,9,10]\n",
    "# imis = [50, 51, 52]\n",
    "# for ai_tag in [False]:\n",
    "#     for imi in imis:\n",
    "#         files, out_dir = get_files(env_name, imi, AI=ai_tag, tdmpc=True)\n",
    "#         if files:\n",
    "#             big_data_dict = files_to_data_dict(files)\n",
    "#             print(f\"Attempting to write to {out_dir}\")\n",
    "#             stats = fastrl_to_hf(big_data_dict, out_dir)\n",
    "#             # for k,v in stats.items():\n",
    "#             #     print(k, v)\n",
    "#         else: print(f\"Could not find files for {imi} AI {ai_tag}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "def resize_images(bdd):\n",
    "    for k in ['pixels', 'image']:\n",
    "        if k in bdd:\n",
    "            print(f\"Original {k} shape:\", bdd[k].shape)\n",
    "\n",
    "            # Reshape if necessary (assuming the images are in NHWC format)\n",
    "            if bdd[k].shape[-1] != 3:\n",
    "                bdd[k] = np.transpose(bdd[k], (0, 2, 3, 1))\n",
    "        \n",
    "            # Get the original dimensions\n",
    "            n, h, w, c = bdd[k].shape\n",
    "        \n",
    "            # Resize to 96x96\n",
    "            resized = np.zeros((n, 96, 96, c), dtype=bdd[k].dtype)\n",
    "            for i in range(n):\n",
    "                resized[i] = cv2.resize(bdd[k][i], (96, 96), interpolation=cv2.INTER_CUBIC)\n",
    "        \n",
    "        # Update the dictionary with resized images\n",
    "            bdd[k] = resized\n",
    "            \n",
    "            print(f\"Resized {k} shape:\", bdd[k].shape)\n",
    "    else:\n",
    "        print(f\"Key '{k}' not found in big_data_dict\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for +: 'PosixPath' and 'str'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m imi \u001b[38;5;129;01min\u001b[39;00m imis:\n\u001b[1;32m      6\u001b[0m     files, out_dir \u001b[38;5;241m=\u001b[39m get_files(env_name, imi, AI\u001b[38;5;241m=\u001b[39mai_tag)\n\u001b[0;32m----> 7\u001b[0m     out_dir \u001b[38;5;241m=\u001b[39m \u001b[43mout_dir\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m_96x96\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m \u001b[38;5;28;01mif\u001b[39;00m RESIZE_TO_96x96 \u001b[38;5;28;01melse\u001b[39;00m out_dir\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m files:\n\u001b[1;32m      9\u001b[0m         big_data_dict \u001b[38;5;241m=\u001b[39m files_to_data_dict(files)\n",
      "\u001b[0;31mTypeError\u001b[0m: unsupported operand type(s) for +: 'PosixPath' and 'str'"
     ]
    }
   ],
   "source": [
    "# imis = [4, 5, 6, 7, 9, 10] if AI else [3,4,5,6,7,8,9,10]\n",
    "RESIZE_TO_96x96 = True\n",
    "imis = [11, 12, 13, 14]\n",
    "for ai_tag in [True, False]:\n",
    "    for imi in imis:\n",
    "        files, out_dir = get_files(env_name, imi, AI=ai_tag, resize=RESIZE_TO_96x96)\n",
    "        if files:\n",
    "            big_data_dict = files_to_data_dict(files)\n",
    "            print(f\"Attempting to write to {out_dir}\")\n",
    "            stats = fastrl_to_hf(big_data_dict, out_dir)\n",
    "            # for k,v in stats.items():\n",
    "            #     print(k, v)\n",
    "        else: print(f\"Could not find files for {imi} AI {ai_tag}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "observation.image 301 246\n",
      "observation.state torch.Size([301, 2]) torch.Size([246, 2])\n",
      "action torch.Size([301, 2]) torch.Size([246, 2])\n",
      "episode_index torch.Size([301]) torch.Size([246])\n",
      "frame_index torch.Size([301]) torch.Size([246])\n",
      "timestamp torch.Size([301]) torch.Size([246])\n",
      "next.reward torch.Size([301]) torch.Size([246])\n",
      "next.done torch.Size([301]) torch.Size([246])\n"
     ]
    }
   ],
   "source": [
    "video = False; fps = 20; video_path = None; debug = False\n",
    "ep_dicts = []\n",
    "episode_data_index = {\"from\": [], \"to\": []}\n",
    "\n",
    "id_from = 0\n",
    "id_to = 0\n",
    "ep_idx = 0\n",
    "data = big_data_dict\n",
    "total_frames = data[\"action\"].shape[0]\n",
    "# for i in tqdm.tqdm(range(total_frames)):\n",
    "for i in range(total_frames):\n",
    "    id_to += 1\n",
    "\n",
    "    if not data[\"is_terminal\"][i]:\n",
    "        continue\n",
    "\n",
    "# print(\"found terminal step\")\n",
    "\n",
    "    num_frames = id_to - id_from\n",
    "\n",
    "    image = torch.tensor(data[\"image\"][id_from:id_to])\n",
    "# image = einops.rearrange(image, \"b h w c -> b h w c\")\n",
    "# image = einops.rearrange(image, \"b c h w -> b h w c\")\n",
    "    state = torch.tensor(data[\"state\"][id_from:id_to, :2]) if (\"state\" in data) else torch.zeros(num_frames, 1)\n",
    "# state = torch.tensor(data[\"vector_state\"][id_from:id_to]) if (\"vector_state\" in data) else torch.zeros(num_frames, 1)\n",
    "    action = (torch.tensor(data[\"action\"][id_from:id_to]) + 1) * 256\n",
    "# action = torch.tensor(data[\"action\"][id_from:id_to])\n",
    "# TODO(rcadene): we have a missing last frame which is the observation when the env is done\n",
    "# it is critical to have this frame for tdmpc to predict a \"done observation/state\"\n",
    "# next_image = torch.tensor(data[\"next_observations\"][\"rgb\"][id_from:id_to])\n",
    "# next_state = torch.tensor(data[\"next_observations\"][\"state\"][id_from:id_to])\n",
    "    next_reward = torch.tensor(data[\"reward\"][id_from:id_to])\n",
    "    next_done = torch.tensor(data[\"is_terminal\"][id_from:id_to])\n",
    "\n",
    "    ep_dict = {}\n",
    "\n",
    "    imgs_array = [x.numpy() for x in image]\n",
    "    img_key = \"observation.image\"\n",
    "    if video:\n",
    "    # save png images in temporary directory\n",
    "        tmp_imgs_dir = out_dir / \"tmp_images\"\n",
    "        tmp_imgs_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        for i in range(len(imgs_array)):\n",
    "            img = PILImage.fromarray(imgs_array[i])\n",
    "            img.save(str(tmp_imgs_dir / f\"frame_{i:06d}.png\"), quality=100)\n",
    "\n",
    "    # encode images to a mp4 video\n",
    "        fname = f\"{img_key}_episode_{ep_idx:06d}.mp4\"\n",
    "        video_path = out_dir / \"videos\" / fname\n",
    "        encode_video_frames(tmp_imgs_dir, video_path, fps)\n",
    "\n",
    "    # clean temporary images directory\n",
    "        shutil.rmtree(tmp_imgs_dir)\n",
    "\n",
    "    # store the reference to the video frame\n",
    "        ep_dict[img_key] = [{\"path\": f\"videos/{fname}\", \"timestamp\": i / fps} for i in range(num_frames)]\n",
    "    else:\n",
    "    # ep_dict[img_key] = [PILImage.fromarray(x) for x in imgs_array]\n",
    "        ep_dict[img_key] = imgs_array\n",
    "\n",
    "    ep_dict[\"observation.state\"] = state\n",
    "    ep_dict[\"action\"] = action\n",
    "    ep_dict[\"episode_index\"] = torch.tensor([ep_idx] * num_frames, dtype=torch.int64)\n",
    "    ep_dict[\"frame_index\"] = torch.arange(0, num_frames, 1)\n",
    "    ep_dict[\"timestamp\"] = torch.arange(0, num_frames, 1) / fps\n",
    "# ep_dict[\"next.observation.image\"] = next_image\n",
    "# ep_dict[\"next.observation.state\"] = next_state\n",
    "    ep_dict[\"next.reward\"] = next_reward\n",
    "    ep_dict[\"next.done\"] = next_done\n",
    "    ep_dicts.append(ep_dict)\n",
    "\n",
    "    episode_data_index[\"from\"].append(id_from)\n",
    "    episode_data_index[\"to\"].append(id_from + num_frames)\n",
    "\n",
    "    id_from = id_to\n",
    "    ep_idx += 1\n",
    "\n",
    "# process first episode only\n",
    "    if debug:\n",
    "        break\n",
    "if len(ep_dicts) == 0:\n",
    "    print(\"No terminal step found in the dataset\")\n",
    "else:\n",
    "    for k,v in ep_dicts[0].items():\n",
    "        print(k, ep_dicts[0][k].shape if hasattr(ep_dicts[0][k], 'shape') else len(ep_dicts[0][k]), ep_dicts[-1][k].shape if hasattr(ep_dicts[-1][k], 'shape') else len(ep_dicts[-1][k]))\n",
    "\n",
    "    # convert things to\n",
    "    data_dict = concatenate_episodes(ep_dicts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 301 torch.Size([301, 2])\n",
      "301 589 torch.Size([288, 2])\n",
      "589 820 torch.Size([231, 2])\n",
      "820 1007 torch.Size([187, 2])\n",
      "1007 1130 torch.Size([123, 2])\n",
      "1130 1412 torch.Size([282, 2])\n",
      "1412 1713 torch.Size([301, 2])\n",
      "1713 2014 torch.Size([301, 2])\n",
      "2014 2315 torch.Size([301, 2])\n",
      "2315 2616 torch.Size([301, 2])\n",
      "2616 2764 torch.Size([148, 2])\n",
      "2764 2949 torch.Size([185, 2])\n",
      "2949 3088 torch.Size([139, 2])\n",
      "3088 3242 torch.Size([154, 2])\n",
      "3242 3444 torch.Size([202, 2])\n",
      "3444 3549 torch.Size([105, 2])\n",
      "3549 3850 torch.Size([301, 2])\n",
      "3850 4113 torch.Size([263, 2])\n",
      "4113 4261 torch.Size([148, 2])\n",
      "4261 4537 torch.Size([276, 2])\n",
      "4537 4790 torch.Size([253, 2])\n",
      "4790 5002 torch.Size([212, 2])\n",
      "5002 5303 torch.Size([301, 2])\n",
      "5303 5387 torch.Size([84, 2])\n",
      "5387 5688 torch.Size([301, 2])\n",
      "5688 5895 torch.Size([207, 2])\n",
      "5895 6026 torch.Size([131, 2])\n",
      "6026 6205 torch.Size([179, 2])\n",
      "6205 6506 torch.Size([301, 2])\n",
      "6506 6727 torch.Size([221, 2])\n",
      "6727 6984 torch.Size([257, 2])\n",
      "6984 7125 torch.Size([141, 2])\n",
      "7125 7251 torch.Size([126, 2])\n",
      "7251 7552 torch.Size([301, 2])\n",
      "7552 7853 torch.Size([301, 2])\n",
      "7853 8154 torch.Size([301, 2])\n",
      "8154 8455 torch.Size([301, 2])\n",
      "8455 8691 torch.Size([236, 2])\n",
      "8691 8992 torch.Size([301, 2])\n",
      "8992 9168 torch.Size([176, 2])\n",
      "9168 9283 torch.Size([115, 2])\n",
      "9283 9584 torch.Size([301, 2])\n",
      "9584 9885 torch.Size([301, 2])\n",
      "9885 9994 torch.Size([109, 2])\n",
      "9994 10289 torch.Size([295, 2])\n",
      "10289 10397 torch.Size([108, 2])\n",
      "10397 10698 torch.Size([301, 2])\n",
      "10698 10978 torch.Size([280, 2])\n",
      "10978 11279 torch.Size([301, 2])\n",
      "11279 11525 torch.Size([246, 2])\n"
     ]
    }
   ],
   "source": [
    "ep_dicts[0]['observation.image'][0].shape\n",
    "\n",
    "\n",
    "for f,t in zip(episode_data_index['from'], episode_data_index['to']):\n",
    "    print(f, t, data_dict['action'][f:t].shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
