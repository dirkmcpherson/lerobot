{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from pprint import pprint\n",
    "\n",
    "import lerobot\n",
    "from lerobot.common.datasets.lerobot_dataset import LeRobotDataset\n",
    "from lerobot.common.datasets.factory import make_dataset\n",
    "\n",
    "\n",
    "from hydra import compose, initialize\n",
    "from omegaconf import OmegaConf\n",
    "\n",
    "# context initialization\n",
    "with initialize(version_base=None, config_path=\"../configs\", job_name=\"test_app\"):\n",
    "    cfg = compose(config_name=\"default\")\n",
    "    print(OmegaConf.to_yaml(cfg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the path to the dataset\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "env_name = 'pusht' # 'pinpad' # 'robosuite'\n",
    "\n",
    "# base_path = Path(f\"~/workspace/lerobot/local/{env_name}/original\").expanduser()\n",
    "# base_path = Path(f\"~/workspace/fastrl/logs/HD_pinpad_four_1/a\").expanduser()\n",
    "imi = 4\n",
    "\n",
    "def get_files(env_name, imi):\n",
    "    base_path = Path(f\"~/workspace/fastrl/logs/HD_pusht_{imi}/\").expanduser()\n",
    "    out_dir = Path(f\"~/workspace/lerobot/local/{env_name}/{imi}\").expanduser()\n",
    "\n",
    "# list all the files in the dataset\n",
    "    folders = list(base_path.glob(\"*\"))\n",
    "\n",
    "    files = []\n",
    "    for f in folders:\n",
    "        files.extend((base_path / f).glob(\"*\"))\n",
    "    return files, out_dir\n",
    "\n",
    "files, out_dir = get_files(env_name, imi)\n",
    "\n",
    "print(files)\n",
    "\n",
    "# print the keys\n",
    "data = np.load(files[0])\n",
    "# convert to a dictionary NOTE: this is necessary to make the arrays writeable for some reason\n",
    "data = dict(data)\n",
    "for k,v in data.items():\n",
    "    print(k, v.shape)\n",
    "\n",
    "# print(\"Setting last is_terminal to true\")\n",
    "# data[\"is_terminal\"][-1] = True; data['is_last'][-1] = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm\n",
    "import torch\n",
    "import einops\n",
    "import shutil\n",
    "from PIL import Image as PILImage\n",
    "import cv2\n",
    "\n",
    "from lerobot.common.datasets.push_dataset_to_hub.utils import concatenate_episodes, save_images_concurrently\n",
    "from lerobot.common.datasets.compute_stats import compute_stats\n",
    "from lerobot.scripts.push_dataset_to_hub import save_meta_data\n",
    "from lerobot.common.datasets.video_utils import VideoFrame, encode_video_frames\n",
    "from lerobot.common.datasets.utils import hf_transform_to_torch\n",
    "from datasets import Dataset, Features, Image, Sequence, Value\n",
    "\n",
    "def to_hf_dataset(data_dict, video):\n",
    "    features = {}\n",
    "\n",
    "    if video:\n",
    "        features[\"observation.image\"] = VideoFrame()\n",
    "    else:\n",
    "        features[\"observation.image\"] = Image()\n",
    "\n",
    "    features[\"observation.state\"] = Sequence(\n",
    "        length=data_dict[\"observation.state\"].shape[1], feature=Value(dtype=\"float32\", id=None)\n",
    "    )\n",
    "    features[\"action\"] = Sequence(\n",
    "        length=data_dict[\"action\"].shape[1], feature=Value(dtype=\"float32\", id=None)\n",
    "    )\n",
    "    features[\"episode_index\"] = Value(dtype=\"int64\", id=None)\n",
    "    features[\"frame_index\"] = Value(dtype=\"int64\", id=None)\n",
    "    features[\"timestamp\"] = Value(dtype=\"float32\", id=None)\n",
    "    features[\"next.reward\"] = Value(dtype=\"float32\", id=None)\n",
    "    features[\"next.done\"] = Value(dtype=\"bool\", id=None)\n",
    "    features[\"index\"] = Value(dtype=\"int64\", id=None)\n",
    "    # TODO(rcadene): add success\n",
    "    # features[\"next.success\"] = Value(dtype='bool', id=None)\n",
    "\n",
    "    hf_dataset = Dataset.from_dict(data_dict, features=Features(features))\n",
    "    hf_dataset.set_transform(hf_transform_to_torch)\n",
    "    return hf_dataset\n",
    "\n",
    "def files_to_data_dict(files):\n",
    "    data_dicts = []\n",
    "    for data_fn in files:\n",
    "        print(f\"Processing {data_fn}\")\n",
    "        data = np.load(data_fn)\n",
    "        data = dict(data); \n",
    "        data[\"is_terminal\"][-1] = True\n",
    "        data_dicts.append(data)\n",
    "    big_data_dict = {}\n",
    "    for k in data_dicts[0].keys():\n",
    "        big_data_dict[k] = np.concatenate([d[k] for d in data_dicts], axis=0)\n",
    "        print(k, big_data_dict[k].shape)\n",
    "    return big_data_dict\n",
    "\n",
    "big_data_dict = files_to_data_dict(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fastrl_to_hf(big_data_dict, out_dir):\n",
    "    video = False; fps = 20; video_path = None; debug = False\n",
    "    ep_dicts = []\n",
    "    episode_data_index = {\"from\": [], \"to\": []}\n",
    "\n",
    "    id_from = 0\n",
    "    id_to = 0\n",
    "    ep_idx = 0\n",
    "    data = big_data_dict\n",
    "    total_frames = data[\"action\"].shape[0]\n",
    "# for i in tqdm.tqdm(range(total_frames)):\n",
    "    for i in range(total_frames):\n",
    "        id_to += 1\n",
    "\n",
    "        if not data[\"is_terminal\"][i]:\n",
    "            continue\n",
    "\n",
    "    # print(\"found terminal step\")\n",
    "\n",
    "        num_frames = id_to - id_from\n",
    "\n",
    "        image = torch.tensor(data[\"image\"][id_from:id_to])\n",
    "    # image = einops.rearrange(image, \"b h w c -> b h w c\")\n",
    "    # image = einops.rearrange(image, \"b c h w -> b h w c\")\n",
    "        state = torch.tensor(data[\"state\"][id_from:id_to, :2]) if (\"state\" in data) else torch.zeros(num_frames, 1)\n",
    "    # state = torch.tensor(data[\"vector_state\"][id_from:id_to]) if (\"vector_state\" in data) else torch.zeros(num_frames, 1)\n",
    "        action = (torch.tensor(data[\"action\"][id_from:id_to]) + 1) * 256\n",
    "    # action = torch.tensor(data[\"action\"][id_from:id_to])\n",
    "    # TODO(rcadene): we have a missing last frame which is the observation when the env is done\n",
    "    # it is critical to have this frame for tdmpc to predict a \"done observation/state\"\n",
    "    # next_image = torch.tensor(data[\"next_observations\"][\"rgb\"][id_from:id_to])\n",
    "    # next_state = torch.tensor(data[\"next_observations\"][\"state\"][id_from:id_to])\n",
    "        next_reward = torch.tensor(data[\"reward\"][id_from:id_to])\n",
    "        next_done = torch.tensor(data[\"is_terminal\"][id_from:id_to])\n",
    "\n",
    "        ep_dict = {}\n",
    "\n",
    "        imgs_array = [x.numpy() for x in image]\n",
    "        img_key = \"observation.image\"\n",
    "        if video:\n",
    "        # save png images in temporary directory\n",
    "            tmp_imgs_dir = out_dir / \"tmp_images\"\n",
    "            tmp_imgs_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "            for i in range(len(imgs_array)):\n",
    "                img = PILImage.fromarray(imgs_array[i])\n",
    "                img.save(str(tmp_imgs_dir / f\"frame_{i:06d}.png\"), quality=100)\n",
    "\n",
    "        # encode images to a mp4 video\n",
    "            fname = f\"{img_key}_episode_{ep_idx:06d}.mp4\"\n",
    "            video_path = out_dir / \"videos\" / fname\n",
    "            encode_video_frames(tmp_imgs_dir, video_path, fps)\n",
    "\n",
    "        # clean temporary images directory\n",
    "            shutil.rmtree(tmp_imgs_dir)\n",
    "\n",
    "        # store the reference to the video frame\n",
    "            ep_dict[img_key] = [{\"path\": f\"videos/{fname}\", \"timestamp\": i / fps} for i in range(num_frames)]\n",
    "        else:\n",
    "        # ep_dict[img_key] = [PILImage.fromarray(x) for x in imgs_array]\n",
    "            ep_dict[img_key] = imgs_array\n",
    "\n",
    "        ep_dict[\"observation.state\"] = state\n",
    "        ep_dict[\"action\"] = action\n",
    "        ep_dict[\"episode_index\"] = torch.tensor([ep_idx] * num_frames, dtype=torch.int64)\n",
    "        ep_dict[\"frame_index\"] = torch.arange(0, num_frames, 1)\n",
    "        ep_dict[\"timestamp\"] = torch.arange(0, num_frames, 1) / fps\n",
    "    # ep_dict[\"next.observation.image\"] = next_image\n",
    "    # ep_dict[\"next.observation.state\"] = next_state\n",
    "        ep_dict[\"next.reward\"] = next_reward\n",
    "        ep_dict[\"next.done\"] = next_done\n",
    "        ep_dicts.append(ep_dict)\n",
    "\n",
    "        episode_data_index[\"from\"].append(id_from)\n",
    "        episode_data_index[\"to\"].append(id_from + num_frames)\n",
    "\n",
    "        id_from = id_to\n",
    "        ep_idx += 1\n",
    "\n",
    "    # process first episode only\n",
    "        if debug:\n",
    "            break\n",
    "    if len(ep_dicts) == 0:\n",
    "        print(\"No terminal step found in the dataset\")\n",
    "    else:\n",
    "        data_dict = concatenate_episodes(ep_dicts)\n",
    "        data_dict, episode_data_index\n",
    "\n",
    "        for k,v in data_dict.items():\n",
    "            print(k, v.shape if hasattr(v, 'shape') else len(v))\n",
    "\n",
    "        hf_dataset = to_hf_dataset(data_dict, video)\n",
    "\n",
    "        info = {\"fps\": fps, \"video\": video}\n",
    "\n",
    "        if video_path: \n",
    "            print(f\"video path: {video_path}\")\n",
    "        lerobot_dataset = LeRobotDataset.from_preloaded(\n",
    "            repo_id=env_name,\n",
    "            hf_dataset=hf_dataset,\n",
    "            episode_data_index=episode_data_index,\n",
    "            info=info,\n",
    "            videos_dir=video_path,\n",
    "            )\n",
    "\n",
    "\n",
    "        hf_dataset = hf_dataset.with_format(None)  # to remove transforms that cant be saved\n",
    "        hf_dataset.save_to_disk(str(out_dir / \"train\"))\n",
    "    print(lerobot_dataset)\n",
    "    stats = compute_stats(lerobot_dataset, batch_size=16, num_workers=1)\n",
    "    save_meta_data(info, stats, episode_data_index, out_dir / \"meta_data\")\n",
    "    return stats\n",
    "\n",
    "stats = fastrl_to_hf(big_data_dict, out_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for imi in [3,4,5,6,7,8,9,10]:\n",
    "    files, out_dir = get_files(env_name, imi)\n",
    "    big_data_dict = files_to_data_dict(files)\n",
    "    stats = fastrl_to_hf(big_data_dict, out_dir)\n",
    "    for k,v in stats.items():\n",
    "        print(k, v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# from datasets import load_dataset, load_from_disk\n",
    "# loaded_dataset = load_from_disk(str(out_dir / \"train\"))\n",
    "\n",
    "# dataloader = torch.utils.data.DataLoader(\n",
    "#     loaded_dataset,\n",
    "#     num_workers=1,\n",
    "#     batch_size=2,\n",
    "#     shuffle=False,\n",
    "# )\n",
    "\n",
    "# batch = next(iter(dataloader))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
