{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from pprint import pprint\n",
    "\n",
    "import lerobot\n",
    "from lerobot.common.datasets.lerobot_dataset import LeRobotDataset\n",
    "from lerobot.common.datasets.factory import make_dataset\n",
    "\n",
    "\n",
    "from hydra import compose, initialize\n",
    "from omegaconf import OmegaConf\n",
    "\n",
    "# context initialization\n",
    "with initialize(version_base=None, config_path=\"../configs\", job_name=\"test_app\"):\n",
    "    cfg = compose(config_name=\"default\")\n",
    "    print(OmegaConf.to_yaml(cfg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lerobot.common.datasets.lerobot_dataset import LeRobotDataset, LeRobotDatasetMetadata\n",
    "import shutil\n",
    "imi = 0\n",
    "use_images = True\n",
    "filter_zeros = False\n",
    "include_failures = False\n",
    "notes = 'image' if use_images else 'vstate'\n",
    "notes += '_zeros' if filter_zeros else ''\n",
    "notes += '_failures' if include_failures else ''\n",
    "\n",
    "repo_id = f\"j/{imi}\"\n",
    "root = Path(f'~/workspace/lerobot/local/ros_{imi}_{notes}').expanduser()\n",
    "dst = Path(f'~/.cache/huggingface/hub/datasets--ros_{imi}_{notes}').expanduser()\n",
    "\n",
    "if dst.exists(): shutil.rmtree(dst)\n",
    "if root.exists(): shutil.rmtree(root)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import rosbag\n",
    "import rospy\n",
    "from kortex_driver.msg import BaseCyclic_Feedback, TwistCommand\n",
    "from sensor_msgs.msg import Image as RosImage, Joy, JointState\n",
    "from std_msgs.msg import Float32, Int8\n",
    "import std_msgs.msg\n",
    "from sensor_msgs.msg import Image, Joy\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "import cv2\n",
    "from cv_bridge import CvBridge\n",
    "\n",
    "bridge = CvBridge()\n",
    "\n",
    "def state_from_basefeedback(msg):\n",
    "    gripper_pos = msg.interconnect.oneof_tool_feedback.gripper_feedback[0].motor[0].position\n",
    "    tool_pose = msg.base.tool_pose_x, msg.base.tool_pose_y, msg.base.tool_pose_z, msg.base.tool_pose_theta_x, msg.base.tool_pose_theta_y, msg.base.tool_pose_theta_z\n",
    "    # return [*tool_pose, gripper_pos]\n",
    "    return [gripper_pos]\n",
    "\n",
    "def state_from_jointstate(msg):\n",
    "    return msg.position[:6] # only first 6 joints, gripper from basefeedback\n",
    "\n",
    "def action_from_joy(msg):\n",
    "    return -msg.buttons[4] if msg.buttons[4] else msg.buttons[5]\n",
    "\n",
    "\n",
    "def action_from_outvel(msg):\n",
    "    return [msg.twist.linear_x, msg.twist.linear_y, msg.twist.linear_z, msg.twist.angular_x, msg.twist.angular_y, msg.twist.angular_z]\n",
    "\n",
    "topic_to_fn = {\n",
    "    '/my_gen3_lite/base_feedback': state_from_basefeedback,\n",
    "    '/my_gen3_lite/joint_states': state_from_jointstate,\n",
    "    '/joy': action_from_joy,\n",
    "    '/my_gen3_lite/in/cartesian_velocity': action_from_outvel,\n",
    "    '/camera_obs__dev_video4_96x96': lambda x: bridge.imgmsg_to_cv2(x, \"bgr8\"),\n",
    "}\n",
    "\n",
    "action_dims = 7\n",
    "state_dims = 7\n",
    "\n",
    "path = Path('~/user_310').expanduser() # sample to pull out state sizes\n",
    "bag = rosbag.Bag(path / 'trial_data.bag')\n",
    "msg_topics = set(); t0 = None; hz = 10; frame = defaultdict(list)\n",
    "video_frames = []\n",
    "\n",
    "for topic, msg, t in bag.read_messages():\n",
    "    if not t0: t0 = t.to_sec()\n",
    "    if t.to_sec() - t0 > 1/hz:\n",
    "        # create a frame\n",
    "        \n",
    "\n",
    "        # Reset t0\n",
    "        t0 = t.to_sec()\n",
    "        frame = defaultdict(list)\n",
    "\n",
    "    if topic not in msg_topics:\n",
    "        # print(topic, type(msg)) #, msg)\n",
    "        msg_topics.add(topic)\n",
    "\n",
    "    if topic in topic_to_fn:\n",
    "        frame[topic].append(topic_to_fn[topic](msg))\n",
    "\n",
    "    # if topic == '/my_gen3_lite/in/cartesian_velocity':\n",
    "    #     print(topic, msg.twist)\n",
    "\n",
    "    elif topic == '/camera_obs__dev_video4_96x96':\n",
    "        \n",
    "        video_frames.append(msg)\n",
    "\n",
    "print(f'bag runtime {t.to_sec() - t0:1.2f} seconds')\n",
    "\n",
    "video = cv2.VideoWriter(str(path / 'video.avi'), cv2.VideoWriter_fourcc(*'XVID'), 10, (96, 96))\n",
    "for i, frame in enumerate(video_frames):\n",
    "    cv2_img = bridge.imgmsg_to_cv2(frame, \"bgr8\")\n",
    "    video.write(cv2_img)\n",
    "    # if i > 100: break\n",
    "video.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_ndims = 7\n",
    "action_ndims = 7\n",
    "\n",
    "features = {\n",
    "    \"observation.state\": {\n",
    "        \"dtype\": \"float32\",\n",
    "        \"shape\": (state_ndims,),\n",
    "        \"names\": [f's{i}' for i in range(state_ndims)],\n",
    "    },\n",
    "    \"action\": {\n",
    "        \"dtype\": \"float32\",\n",
    "        \"shape\": (action_ndims,),\n",
    "        \"names\": [f'a{i}' for i in range(action_ndims)],\n",
    "    },\n",
    "    \"next.reward\": {\n",
    "        \"dtype\": \"float32\",\n",
    "        \"shape\": (1,),\n",
    "        \"names\": None,\n",
    "    },\n",
    "    \"next.success\": {\n",
    "        \"dtype\": \"bool\",\n",
    "        \"shape\": (1,),\n",
    "        \"names\": None,\n",
    "    },\n",
    "}\n",
    "\n",
    "if use_images:\n",
    "    features[\"observation.image.top\"] = {\n",
    "        \"dtype\": \"image\",\n",
    "        \"shape\": (3, 96, 96),\n",
    "        \"names\": [\n",
    "            \"channel\",\n",
    "            \"height\",\n",
    "            \"width\",\n",
    "        ],\n",
    "        'fps': 10\n",
    "    }\n",
    "else:\n",
    "    features[\"observation.environment_state\"] = {\n",
    "        \"dtype\": \"float32\",\n",
    "        \"shape\": (2,),\n",
    "        \"names\": [f'env_s{i}' for i in range(state_ndims)],\n",
    "    },\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "trials_position_0_successful = [232, 235, 242, 245, 248, 251, 254, 257, 261, 265, 269, 273, 276, 279, 283, 293, 297, 301, 304, 308, 315, 316, 319, 320, 325, 328, 331, 335]\n",
    "trials_position_0_failed = [238, 249, 260, 268, 282, 288, 307, 312, 324, 331, 334]\n",
    "\n",
    "trials_position_1_successful = [233, 236, 239, 243, 246, 252, 255, 258, 262, 266, 274, 277, 280, 284, 287, 294, 298, 302, 305, 309, 317, 321, 326, 329, 322]\n",
    "trials_position_1_failed = [270, 313]\n",
    "\n",
    "trials_position_2_successful = [327, 330, 333, 300, 303, 306, 311, 318, 278, 281, 286, 290, 295, 299, 256, 259, 263, 267, 275, 234, 237, 244, 247, 250]\n",
    "trials_position_2_failed = [240, 253, 285, 289, 296, 310, 314, 322]\n",
    "\n",
    "trials = trials_position_0_successful + trials_position_1_successful + trials_position_2_successful\n",
    "if include_failures:\n",
    "    trials += trials_position_0_failed + trials_position_1_failed + trials_position_2_failed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if dst.exists(): shutil.rmtree(dst)\n",
    "if root.exists(): shutil.rmtree(root)\n",
    "\n",
    "# metadata = LeRobotDatasetMetadata(repo_id, root, local_files_only=True)\n",
    "dataset = LeRobotDataset.create(\n",
    "    repo_id,\n",
    "    fps=hz, # from pusht.yaml\n",
    "    root=root,\n",
    "    use_videos=True,\n",
    "    features=features\n",
    ")\n",
    "msg_topics = set(); must_have_keys = [entry for entry in list(topic_to_fn.keys()) if entry not in ['/my_gen3_lite/in/cartesian_velocity', '/joy']]\n",
    "zeros_filtered = 0\n",
    "for uid in trials: # just the position 2 successes\n",
    "    path = Path(f'~/user_{uid}').expanduser() # sample to pull out state sizes\n",
    "    bag = rosbag.Bag(path / 'trial_data.bag')\n",
    "\n",
    "    t0 = None; hz = 10; frame = defaultdict(list); bag_start = None; total_frames = 0\n",
    "\n",
    "    all_actions = []\n",
    "    all_states = []\n",
    "    all_video_frames = []\n",
    "\n",
    "    for topic, msg, t in bag.read_messages(): # NOTE: we're dropping the last frame\n",
    "        if not t0: t0 = t.to_sec()\n",
    "        if not bag_start: bag_start = t.to_sec()\n",
    "        # print(f'{t.to_sec() - bag_start:1.2f}')\n",
    "        # if topic == '/my_gen3_lite/in/cartesian_velocity':\n",
    "        #     print('\\t', msg.twist)\n",
    "\n",
    "        dt = t.to_sec() - t0\n",
    "        if dt >= 1/hz:\n",
    "            # print(f'{dt=:1.2f}')\n",
    "            # create a frame\n",
    "\n",
    "            # Action and state are the mean of the frames\n",
    "            # only make a frame if we have data of all topics\n",
    "            if all(len(frame[k]) > 0 for k in must_have_keys):\n",
    "                LR_frame = {}\n",
    "                LR_frame['next.success'] = 0\n",
    "                LR_frame['next.reward'] = 0\n",
    "\n",
    "                joint_state_mean = np.mean(frame['/my_gen3_lite/joint_states'], axis=0)\n",
    "                gripper_pos = np.mean(frame['/my_gen3_lite/base_feedback'], axis=0)\n",
    "                LR_frame['observation.state'] = np.concatenate([joint_state_mean, gripper_pos])\n",
    "\n",
    "                if '/my_gen3_lite/in/cartesian_velocity' in frame:\n",
    "                    joint_action = np.mean(frame['/my_gen3_lite/in/cartesian_velocity'], axis=0)\n",
    "                else:\n",
    "                    joint_action = np.zeros((6,))\n",
    "\n",
    "                if '/joy' in frame:\n",
    "                    gripper_action = min(1, max(-1, np.sum(frame['/joy'], axis=0))) # sum the actions since they're -1, 1\n",
    "                else:\n",
    "                    gripper_action = 0\n",
    "\n",
    "                if abs(gripper_action) > 0 or any([abs(entry) for entry in frame['/joy']]):\n",
    "                    print(f'gripper action {gripper_action}', frame['/joy'])\n",
    "\n",
    "                # make sure gripper action is concatenateable \n",
    "                gripper_action = np.array([gripper_action]) \n",
    "                action = np.concatenate([joint_action, gripper_action], axis=0)\n",
    "                \n",
    "                # print(action)\n",
    "                if True and np.all(action == 0):\n",
    "                    zeros_filtered += 1\n",
    "                else:\n",
    "                    LR_frame['action'] = action\n",
    "\n",
    "                    if use_images:\n",
    "                        LR_frame['observation.image.top'] = frame['/camera_obs__dev_video4_96x96'][0]\n",
    "                    else:\n",
    "                        # placeholder state for cup position\n",
    "                        LR_frame['observation.environment_state'] = np.zeros((2,))\n",
    "                    \n",
    "                    dataset.add_frame(LR_frame)\n",
    "                    total_frames += 1\n",
    "\n",
    "                    all_actions.append(LR_frame['action'])\n",
    "                    all_states.append(LR_frame['observation.state'])\n",
    "\n",
    "                    video_frame = frame['/camera_obs__dev_video4_96x96'][0]\n",
    "                    # put action on the video\n",
    "                    cv2_img = video_frame #bridge.imgmsg_to_cv2(video_frame, \"bgr8\")\n",
    "\n",
    "                    # make the frame big\n",
    "                    cv2_img = cv2.resize(cv2_img, (256, 256), interpolation=cv2.INTER_AREA)\n",
    "                    cv2.putText(cv2_img, ', '.join([f'{a:1.2f}' for a in action]), (10, 10), cv2.FONT_HERSHEY_SIMPLEX, 0.3, (0, 255, 0), 1)\n",
    "                    all_video_frames.append(cv2_img)\n",
    "\n",
    "\n",
    "            # Reset t0\n",
    "            t0 = t.to_sec()\n",
    "            frame = defaultdict(list)\n",
    "\n",
    "        if topic not in msg_topics:\n",
    "            print(topic, type(msg)) #, msg)\n",
    "            msg_topics.add(topic)\n",
    "\n",
    "        if topic in topic_to_fn:\n",
    "            val = topic_to_fn[topic](msg)\n",
    "            # if np.isnan(val).any():\n",
    "            #     print(f'nan in {topic} {val}')\n",
    "            frame[topic].append(val)\n",
    "\n",
    "    print(f'bag runtime {t.to_sec() - t0:1.2f} seconds')\n",
    "\n",
    "    video = cv2.VideoWriter(str(path / 'video.avi'), cv2.VideoWriter_fourcc(*'XVID'), 10, (256, 256))\n",
    "    for i, frame in enumerate(all_video_frames):\n",
    "        video.write(frame)\n",
    "        # if i > 100: break\n",
    "    video.release()\n",
    "\n",
    "    dataset.save_episode(\"Pick up a cup.\", encode_videos=False)\n",
    "    print(f'added {total_frames} frames from {uid}')\n",
    "print(f'filtered {zeros_filtered} zeros')\n",
    "dataset.consolidate()\n",
    "dataset.meta.stats['action']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot each action and state by dimension\n",
    "import matplotlib.pyplot as plt\n",
    "fig, axs = plt.subplots(all_actions[0].shape[0], 1, figsize=(10, 10))\n",
    "for i in range(all_actions[0].shape[0]):\n",
    "    # count the nonzero actions\n",
    "    print(np.sum([a[i] != 0 for a in all_actions]))\n",
    "    axs[i].scatter([_ for _ in range(len(all_actions))], [a[i] for a in all_actions])\n",
    "    axs[i].set_title(f'action {i}')\n",
    "\n",
    "fig, axs = plt.subplots(all_states[0].shape[0], 1, figsize=(10, 10))\n",
    "for i in range(all_states[0].shape[0]):\n",
    "    axs[i].plot([a[i] for a in all_states])\n",
    "    axs[i].set_title(f'state {i}')\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
