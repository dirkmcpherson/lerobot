{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from pprint import pprint\n",
    "\n",
    "import lerobot\n",
    "from lerobot.common.datasets.lerobot_dataset import LeRobotDataset\n",
    "from lerobot.common.datasets.factory import make_dataset\n",
    "\n",
    "\n",
    "from hydra import compose, initialize\n",
    "from omegaconf import OmegaConf\n",
    "\n",
    "# context initialization\n",
    "with initialize(version_base=None, config_path=\"../configs\", job_name=\"test_app\"):\n",
    "    cfg = compose(config_name=\"default\")\n",
    "    print(OmegaConf.to_yaml(cfg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lerobot.common.datasets.lerobot_dataset import LeRobotDataset, LeRobotDatasetMetadata\n",
    "import shutil\n",
    "imi = 0\n",
    "use_images = True\n",
    "filter_zeros = True\n",
    "include_failures = False\n",
    "notes = 'image' if use_images else 'vstate'\n",
    "notes += '_zeros' if filter_zeros else ''\n",
    "notes += '_failures' if include_failures else ''\n",
    "\n",
    "repo_id = f\"j/{imi}\"\n",
    "root = Path(f'~/workspace/lerobot/local/ros_{imi}_{notes}').expanduser()\n",
    "dst = Path(f'~/.cache/huggingface/hub/datasets--ros_{imi}_{notes}').expanduser()\n",
    "\n",
    "if dst.exists(): print(f\"Removing {dst}\"); shutil.rmtree(dst)\n",
    "if root.exists(): shutil.rmtree(root); print(f\"Removing {root}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import rosbag\n",
    "import rospy\n",
    "from kortex_driver.msg import BaseCyclic_Feedback, TwistCommand\n",
    "from sensor_msgs.msg import Image as RosImage, Joy, JointState\n",
    "from std_msgs.msg import Float32, Int8\n",
    "import std_msgs.msg\n",
    "from sensor_msgs.msg import Image, Joy\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "import cv2\n",
    "from cv_bridge import CvBridge\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "bridge = CvBridge()\n",
    "\n",
    "def state_from_basefeedback(msg):\n",
    "    gripper_pos = msg.interconnect.oneof_tool_feedback.gripper_feedback[0].motor[0].position\n",
    "    tool_pose = msg.base.tool_pose_x, msg.base.tool_pose_y, msg.base.tool_pose_z, msg.base.tool_pose_theta_x, msg.base.tool_pose_theta_y, msg.base.tool_pose_theta_z\n",
    "    # return [*tool_pose, gripper_pos]\n",
    "    return [gripper_pos]\n",
    "\n",
    "def state_from_jointstate(msg):\n",
    "    return msg.position[:6] # only first 6 joints, gripper from basefeedback\n",
    "\n",
    "def action_from_joy(msg):\n",
    "    return -msg.buttons[4] if msg.buttons[4] else msg.buttons[5]\n",
    "\n",
    "\n",
    "def action_from_outvel(msg):\n",
    "    return [msg.twist.linear_x, msg.twist.linear_y, msg.twist.linear_z, msg.twist.angular_x, msg.twist.angular_y, msg.twist.angular_z]\n",
    "\n",
    "crop_dim = 700\n",
    "crop_left_offset = 200\n",
    "def top_image_to_cv2(cvimg):\n",
    "    cvimg = cvimg[:crop_dim, crop_left_offset:crop_left_offset+crop_dim]\n",
    "    return cvimg\n",
    "\n",
    "def top_image_msg_to_cv2(msg):\n",
    "    cvimg = bridge.imgmsg_to_cv2(msg, \"bgr8\")\n",
    "    return cvimg\n",
    "\n",
    "topic_to_fn = {\n",
    "    '/my_gen3_lite/base_feedback': state_from_basefeedback,\n",
    "    '/my_gen3_lite/joint_states': state_from_jointstate,\n",
    "    '/joy': action_from_joy,\n",
    "    '/my_gen3_lite/in/cartesian_velocity': action_from_outvel,\n",
    "    '/camera_obs__dev_video4_96x96': top_image_msg_to_cv2,\n",
    "}\n",
    "\n",
    "action_dims = 7\n",
    "state_dims = 7\n",
    "\n",
    "path = Path('~/user_315').expanduser() # sample to pull out state sizes\n",
    "bag = rosbag.Bag(path / 'trial_data.bag')\n",
    "msg_topics = set(); t0 = None; hz = 10; frame = defaultdict(list)\n",
    "video_frames = []\n",
    "\n",
    "n_frames = 0\n",
    "for topic, msg, t in bag.read_messages():\n",
    "    if not t0: t0 = t.to_sec()\n",
    "    if t.to_sec() - t0 > 1/hz:\n",
    "        # create a frame\n",
    "        \n",
    "\n",
    "        # Reset t0\n",
    "        t0 = t.to_sec()\n",
    "        frame = defaultdict(list)\n",
    "\n",
    "    if topic not in msg_topics:\n",
    "        # print(topic, type(msg)) #, msg)\n",
    "        msg_topics.add(topic)\n",
    "\n",
    "    if topic in topic_to_fn:\n",
    "        frame[topic].append(topic_to_fn[topic](msg))\n",
    "\n",
    "    # if topic == '/my_gen3_lite/in/cartesian_velocity':\n",
    "    #     print(topic, msg.twist)\n",
    "\n",
    "    if topic == '/camera_obs__dev_video4_96x96':\n",
    "        \n",
    "        video_frames.append(msg)\n",
    "        cv2_img = top_image_msg_to_cv2(msg)\n",
    "        if n_frames % 200 == 0:\n",
    "            plt.imshow(cv2_img)\n",
    "            plt.show()\n",
    "    n_frames += 1\n",
    "\n",
    "print(f'bag runtime {t.to_sec() - t0:1.2f} seconds')\n",
    "\n",
    "video = cv2.VideoWriter(str(path / 'video.avi'), cv2.VideoWriter_fourcc(*'XVID'), 10, (96, 96))\n",
    "for i, frame in enumerate(video_frames):\n",
    "    cv2_img = bridge.imgmsg_to_cv2(frame, \"bgr8\")\n",
    "    video.write(cv2_img)\n",
    "    # if i > 100: break\n",
    "video.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_ndims = 7\n",
    "action_ndims = 7\n",
    "\n",
    "features = {\n",
    "    \"observation.state\": {\n",
    "        \"dtype\": \"float32\",\n",
    "        \"shape\": (state_ndims,),\n",
    "        \"names\": [f's{i}' for i in range(state_ndims)],\n",
    "    },\n",
    "    \"action\": {\n",
    "        \"dtype\": \"float32\",\n",
    "        \"shape\": (action_ndims,),\n",
    "        \"names\": [f'a{i}' for i in range(action_ndims)],\n",
    "    },\n",
    "    \"next.reward\": {\n",
    "        \"dtype\": \"float32\",\n",
    "        \"shape\": (1,),\n",
    "        \"names\": None,\n",
    "    },\n",
    "    \"next.success\": {\n",
    "        \"dtype\": \"bool\",\n",
    "        \"shape\": (1,),\n",
    "        \"names\": None,\n",
    "    },\n",
    "}\n",
    "\n",
    "if use_images:\n",
    "    features[\"observation.image.top\"] = {\n",
    "        \"dtype\": \"image\",\n",
    "        \"shape\": (3, 96, 96),\n",
    "        \"names\": [\n",
    "            \"channel\",\n",
    "            \"height\",\n",
    "            \"width\",\n",
    "        ],\n",
    "        'fps': 10\n",
    "    }\n",
    "else:\n",
    "    features[\"observation.environment_state\"] = {\n",
    "        \"dtype\": \"float32\",\n",
    "        \"shape\": (3,),\n",
    "        \"names\": [f'env_s{i}' for i in range(2)],\n",
    "    }\n",
    "\n",
    "for k, v in features.items():\n",
    "    print(k)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "import time\n",
    "class VideoLoader:\n",
    "    def __init__(self, video_dirs, threshold_ns=2e6, cache_size=100):\n",
    "        \"\"\"\n",
    "        Initialize the VideoLoader with paths to videos and their corresponding timestamps.\n",
    "        \"\"\"\n",
    "        self.dirs = [str(entry).split('/')[-1] for entry in video_dirs]\n",
    "        self.video_paths = [Path(dirname).absolute() / 'output.mp4' for dirname in video_dirs]\n",
    "        timestamp_paths = [Path(dirname).absolute() / 'video_frame_timestamps.txt' for dirname in video_dirs]\n",
    "        self.timestamp_lists = []\n",
    "       \n",
    "        for timestamp_fn in timestamp_paths:\n",
    "            timestamps = []\n",
    "            with open(str(timestamp_fn), 'r') as fp:\n",
    "                timestamps = fp.readlines()\n",
    "            timestamps = [rospy.Time.from_seconds(int(t) / 1e9) for t in timestamps]\n",
    "            self.timestamp_lists.append(timestamps)\n",
    "\n",
    "        self.frames = [None for _ in self.video_paths]  # To store frames of each video\n",
    "        self.captures = []\n",
    "        self.frame_caches = [deque(maxlen=cache_size) for _ in self.video_paths]\n",
    "        # self.frame_timestamps = []  # To store timestamp lists of each video\n",
    "        self.frame_idx = [0 for _ in self.video_paths] # NOTE: this class dumps its frames, it doesn't search them, when it runs into problems it yells and skips frames. Its like a bag.\n",
    "        # self._load_videos()\n",
    "        self._open_videos()\n",
    "\n",
    "\n",
    "\n",
    "        self.threshold_ns = rospy.Time(secs=0, nsecs=threshold_ns)\n",
    "\n",
    "    def _open_videos(self):\n",
    "        \"\"\"\n",
    "        Open video files for streaming.\n",
    "        \"\"\"\n",
    "        self.captures = [cv2.VideoCapture(str(path)) for path in self.video_paths]\n",
    "        for i, cap in enumerate(self.captures):\n",
    "            if not cap.isOpened():\n",
    "                raise ValueError(f\"Cannot open video file: {self.video_paths[i]}\")\n",
    "            else:\n",
    "                # load the first frame\n",
    "                ret, frame = cap.read()\n",
    "                if not ret:\n",
    "                    raise ValueError(f\"Cannot read from capture {self.video_paths[i]}\")\n",
    "                self.frames[i] = frame\n",
    "\n",
    "    # NOTE: Better to drop frames than spend too much time getting them (frames will be dropped in the real world)\n",
    "    def get_frame_if_available(self, target_timestamp):\n",
    "        \"\"\"\n",
    "        Retrieve a frame if the next frame is close enough to the passed target_timestamp. NOTE: if you didn't have a fast signal in the rosbag you would miss \n",
    "        \n",
    "        :param target_timestamp: Timestamp for which to retrieve the frame.\n",
    "        :return: Frames at the given target_timestamp or None if no close match.\n",
    "        \"\"\"\n",
    "        t0 = time.perf_counter()\n",
    "        ret_frames = [None for _ in self.frames]\n",
    "        read = 0\n",
    "        percent_complete = {}\n",
    "        for cam_idx, capture in enumerate(self.captures):\n",
    "            ts = self.timestamp_lists[cam_idx][self.frame_idx[cam_idx]] # the time of the current frame\n",
    "            dt = ts.to_nsec() - target_timestamp.to_nsec() # the difference between the current frame and the target timestamp\n",
    "            \n",
    "            if dt < 0:\n",
    "                while dt < -self.threshold_ns.to_nsec(): # if the current frame is too far behind the target timestamp\n",
    "                    ret, frame = capture.read(); read += 1\n",
    "                    self.frames[cam_idx] = frame\n",
    "                    self.frame_idx[cam_idx] += 1\n",
    "                    ts = self.timestamp_lists[cam_idx][self.frame_idx[cam_idx]]\n",
    "                    dt = ts.to_nsec() - target_timestamp.to_nsec() # the difference between the current frame and the target timestamp\n",
    "                ret_frames[cam_idx] = self.frames[cam_idx]; self.frames[cam_idx] = None\n",
    "            else:\n",
    "                if dt > self.threshold_ns.to_nsec(): # if the current frame is too far ahead of the target timestamp\n",
    "                    ret_frames[cam_idx] = None\n",
    "                else:\n",
    "                    ret_frames[cam_idx] = self.frames[cam_idx]; self.frames[cam_idx] = None\n",
    "\n",
    "        \n",
    "            idx = self.frame_idx[cam_idx] \n",
    "            if idx > len(self.timestamp_lists[cam_idx]):\n",
    "                rospy.loginfo(\"Out of video frames for camera {self.video_paths[cam_idx]}\")\n",
    "                continue\n",
    "\n",
    "            remaining_frame_count = len(self.timestamp_lists[cam_idx]) - idx\n",
    "            percent_complete[cam_idx] = 1 - (remaining_frame_count / len(self.timestamp_lists[cam_idx]))\n",
    "\n",
    "        endT = time.perf_counter()\n",
    "        # if (endT - t0) > 0.01:\n",
    "            # rospy.logwarn(f\"Video read took longer than 0.01 seconds: {(endT - t0)=}\")\n",
    "\n",
    "        return ret_frames, ts, percent_complete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trials_position_0_successful = [232, 235, 242, 245, 248, 251, 254, 257, 261, 265, 269, 273, 276, 279, 283, 293, 297, 301, 304, 308, 315, 316, 319, 320, 325, 328, 331, 335]\n",
    "trials_position_0_failed = [238, 249, 260, 268, 282, 288, 307, 312, 324, 331, 334]\n",
    "\n",
    "trials_position_1_successful = [233, 236, 239, 243, 246, 252, 255, 258, 262, 266, 274, 277, 280, 284, 287, 294, 298, 302, 305, 309, 317, 321, 326, 329, 322]\n",
    "trials_position_1_failed = [270, 313]\n",
    "\n",
    "trials_position_2_successful = [327, 330, 333, 300, 303, 306, 311, 318, 278, 281, 286, 290, 295, 299, 256, 259, 263, 267, 275, 234, 237, 244, 247, 250]\n",
    "trials_position_2_failed = [240, 253, 285, 289, 296, 310, 314, 322]\n",
    "\n",
    "trials = trials_position_0_successful + trials_position_1_successful + trials_position_2_successful\n",
    "if include_failures:\n",
    "    trials += trials_position_0_failed + trials_position_1_failed + trials_position_2_failed\n",
    "\n",
    "in0 = lambda x: x in trials_position_0_failed or x in trials_position_0_successful\n",
    "in1 = lambda x: x in trials_position_1_failed or x in trials_position_1_successful\n",
    "in2 = lambda x: x in trials_position_2_failed or x in trials_position_2_successful\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if dst.exists(): shutil.rmtree(dst)\n",
    "if root.exists(): shutil.rmtree(root)\n",
    "\n",
    "# metadata = LeRobotDatasetMetadata(repo_id, root, local_files_only=True)\n",
    "dataset = LeRobotDataset.create(\n",
    "    repo_id,\n",
    "    fps=hz, # from pusht.yaml\n",
    "    root=root,\n",
    "    use_videos=use_images,\n",
    "    features=features\n",
    ")\n",
    "msg_topics = set(); must_have_keys = [entry for entry in list(topic_to_fn.keys()) if entry not in ['/my_gen3_lite/in/cartesian_velocity', '/joy']]\n",
    "zeros_filtered = 0\n",
    "for uid in trials: # just the position 2 successes\n",
    "    path = Path(f'~/user_{uid}').expanduser() # sample to pull out state sizes\n",
    "\n",
    "    video_dirs = [entry for entry in path.iterdir() if \"cam_dev_video\" in str(entry)]\n",
    "    video_loader = VideoLoader(video_dirs)\n",
    "\n",
    "    bag = rosbag.Bag(path / 'trial_data.bag')\n",
    "\n",
    "    t0 = None; hz = 10; frame = defaultdict(list); bag_start = None; total_frames = 0\n",
    "\n",
    "    all_actions = []\n",
    "    all_states = []\n",
    "    all_video_frames = []\n",
    "\n",
    "    for topic, msg, t in bag.read_messages(): # NOTE: we're dropping the last frame\n",
    "        if not t0: t0 = t.to_sec()\n",
    "        if not bag_start: bag_start = t.to_sec()\n",
    "        # print(f'{t.to_sec() - bag_start:1.2f}')\n",
    "        # if topic == '/my_gen3_lite/in/cartesian_velocity':\n",
    "        #     print('\\t', msg.twist)\n",
    "\n",
    "        dt = t.to_sec() - t0\n",
    "        if dt >= 1/hz:\n",
    "            # print(f'{dt=:1.2f}')\n",
    "            # create a frame\n",
    "\n",
    "            # Action and state are the mean of the frames\n",
    "            # only make a frame if we have data of all topics\n",
    "            if all(len(frame[k]) > 0 for k in must_have_keys):\n",
    "                LR_frame = {}\n",
    "                LR_frame['next.success'] = 0\n",
    "                LR_frame['next.reward'] = 0\n",
    "\n",
    "                joint_state_mean = np.mean(frame['/my_gen3_lite/joint_states'], axis=0)\n",
    "                gripper_pos = np.mean(frame['/my_gen3_lite/base_feedback'], axis=0)\n",
    "                LR_frame['observation.state'] = np.concatenate([joint_state_mean, gripper_pos])\n",
    "\n",
    "                if '/my_gen3_lite/in/cartesian_velocity' in frame:\n",
    "                    joint_action = np.mean(frame['/my_gen3_lite/in/cartesian_velocity'], axis=0)\n",
    "                else:\n",
    "                    joint_action = np.zeros((6,))\n",
    "\n",
    "                if '/joy' in frame:\n",
    "                    gripper_action = min(1, max(-1, np.sum(frame['/joy'], axis=0))) # sum the actions since they're -1, 1\n",
    "                else:\n",
    "                    gripper_action = 0\n",
    "\n",
    "                if abs(gripper_action) > 0 or any([abs(entry) for entry in frame['/joy']]):\n",
    "                    print(f'gripper action {gripper_action}', frame['/joy'])\n",
    "\n",
    "                # make sure gripper action is concatenateable \n",
    "                gripper_action = np.array([gripper_action]) \n",
    "                action = np.concatenate([joint_action, gripper_action], axis=0)\n",
    "                \n",
    "                # print(action)\n",
    "                if True and np.all(action == 0):\n",
    "                    zeros_filtered += 1\n",
    "                else:\n",
    "                    LR_frame['action'] = action\n",
    "\n",
    "                    if use_images:\n",
    "                        # LR_frame['observation.image.top'] = frame['/camera_obs__dev_video4_96x96'][0]\n",
    "                        camera_frames, ts, percent_complete = video_loader.get_frame_if_available(t - rospy.Time.from_sec(0.1))\n",
    "                        if camera_frames[1] is None: continue\n",
    "                        top_img = top_image_to_cv2(camera_frames[1])\n",
    "                        top_img = cv2.resize(top_img, (96, 96), interpolation=cv2.INTER_AREA)\n",
    "                        \n",
    "                        mini_frame = frame['/camera_obs__dev_video4_96x96'][0]\n",
    "\n",
    "                        # concatenate the 2 imgs\n",
    "                        toshow = np.concatenate([mini_frame, top_img], axis=1)\n",
    "\n",
    "                        # plt.imshow(toshow)\n",
    "                        # plt.show()\n",
    "                        # break\n",
    "                        LR_frame['observation.image.top'] = top_img\n",
    "                    else:\n",
    "                        # placeholder state for cup position\n",
    "                        env_state = np.zeros((3,))\n",
    "                        if in0(uid): env_state[0] = 1\n",
    "                        elif in1(uid): env_state[1] = 1\n",
    "                        elif in2(uid): env_state[2] = 1\n",
    "                        else: raise ValueError(f'uid {uid} not in any position')\n",
    "                        LR_frame['observation.environment_state'] = env_state\n",
    "                    \n",
    "                    dataset.add_frame(LR_frame)\n",
    "                    total_frames += 1\n",
    "\n",
    "                    all_actions.append(LR_frame['action'])\n",
    "                    all_states.append(LR_frame['observation.state'])\n",
    "\n",
    "                    video_frame = frame['/camera_obs__dev_video4_96x96'][0]\n",
    "                    # put action on the video\n",
    "                    cv2_img = video_frame #bridge.imgmsg_to_cv2(video_frame, \"bgr8\")\n",
    "\n",
    "                    # make the frame big\n",
    "                    cv2_img = cv2.resize(cv2_img, (256, 256), interpolation=cv2.INTER_AREA)\n",
    "                    cv2.putText(cv2_img, ', '.join([f'{a:1.2f}' for a in action]), (10, 10), cv2.FONT_HERSHEY_SIMPLEX, 0.3, (0, 255, 0), 1)\n",
    "                    all_video_frames.append(cv2_img)\n",
    "\n",
    "\n",
    "            # Reset t0\n",
    "            t0 = t.to_sec()\n",
    "            frame = defaultdict(list)\n",
    "\n",
    "        if topic not in msg_topics:\n",
    "            print(topic, type(msg)) #, msg)\n",
    "            msg_topics.add(topic)\n",
    "\n",
    "        if topic in topic_to_fn:\n",
    "            val = topic_to_fn[topic](msg)\n",
    "            # if np.isnan(val).any():\n",
    "            #     print(f'nan in {topic} {val}')\n",
    "            frame[topic].append(val)\n",
    "\n",
    "    print(f'bag runtime {t.to_sec() - t0:1.2f} seconds')\n",
    "\n",
    "    video = cv2.VideoWriter(str(path / 'video.avi'), cv2.VideoWriter_fourcc(*'XVID'), 10, (256, 256))\n",
    "    for i, frame in enumerate(all_video_frames):\n",
    "        video.write(frame)\n",
    "        # if i > 100: break\n",
    "    video.release()\n",
    "\n",
    "    dataset.save_episode(\"Pick up a cup.\", encode_videos=False)\n",
    "    print(f'added {total_frames} frames from {uid}')\n",
    "    # break\n",
    "print(f'filtered {zeros_filtered} zeros')\n",
    "dataset.consolidate()\n",
    "dataset.meta.stats['action']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot each action and state by dimension\n",
    "import matplotlib.pyplot as plt\n",
    "fig, axs = plt.subplots(all_actions[0].shape[0], 1, figsize=(10, 10))\n",
    "for i in range(all_actions[0].shape[0]):\n",
    "    # count the nonzero actions\n",
    "    print(np.sum([a[i] != 0 for a in all_actions]))\n",
    "    axs[i].scatter([_ for _ in range(len(all_actions))], [a[i] for a in all_actions])\n",
    "    axs[i].set_title(f'action {i}')\n",
    "\n",
    "fig, axs = plt.subplots(all_states[0].shape[0], 1, figsize=(10, 10))\n",
    "for i in range(all_states[0].shape[0]):\n",
    "    axs[i].plot([a[i] for a in all_states])\n",
    "    axs[i].set_title(f'state {i}')\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
